This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: docs/**, **/*.TXT
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    tests.yml
bin/
  benchmark-remerge.sh
rust/
  src/
    lib.rs
src/
  remerge/
    __init__.py
    _core.pyi
    core.py
tests/
  __init__.py
  conftest.py
  fixtures.py
  test_remerge.py
  test_smoke.py
.gitattributes
.gitignore
AGENTS.md
Cargo.toml
CITATION.cff
explanation.png
LICENSE
PYO3_practices.md
pyproject.toml
README.md
REMERGE_improvement_plan.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="PYO3_practices.md">
## PyO3 + maturin best practices (with real-world patterns from Polars)

This is a practical “how to do it right” guide for building Rust-backed Python packages with **PyO3** (bindings) and **maturin** (build + packaging). It focuses on patterns that scale from tiny extensions to big, performance-sensitive libraries.

---

## 1) Design goals that drive good architecture

Before you write code, decide what you’re optimizing for:

* **Fast local iteration** (developers rebuilding constantly)
* **Easy installs for users** (many prebuilt wheels; minimal toolchain pain)
* **Performance** (CPU features, minimal copying, low overhead at the boundary)
* **API stability** (Python users will treat your surface as “the product”)
* **Portability** (Linux/macOS/Windows, x86_64/aarch64, manylinux/musllinux, etc.)

These choices affect two early forks:

### Fork A: “pure Rust extension” vs “mixed Python + Rust”

maturin explicitly supports both and expects different layouts. In a **pure Rust** layout, maturin will auto-generate a small `__init__.py` so users can `import my_pkg` cleanly, but it also notes that you can’t currently include arbitrary extra package data that way, pushing larger projects toward a mixed layout. ([Maturin][1])

### Fork B: “abi3 wheels” vs “version-specific wheels”

PyO3 documents `abi3` as a way to shrink your wheel matrix (one wheel works across many CPython versions), but it comes with feature/optimization tradeoffs. PyO3 lists several `abi3` limitations (e.g., buffer API not until Python 3.11+, some class options not until 3.9+, and loss of version-specific optimizations). ([PyO3][2])

---

## 2) Repository / crate organization patterns that age well

### Pattern 1 (smallest): Single crate = extension module

Good for utilities and small bindings.

* `Cargo.toml` builds a `cdylib`
* `src/lib.rs` contains your `#[pymodule]`
* `pyproject.toml` uses maturin as the build backend ([Maturin][3])

Use this when:

* the Python API is thin
* you don’t need extra Python files beyond the extension

### Pattern 2 (most common): “core Rust crate” + “bindings crate”

This is the default best-practice for anything non-trivial.

* `core/` (pure Rust library, no PyO3 dependency)
* `python/` (a separate crate that depends on `core` and exposes PyO3 API)

Why this wins:

* you keep Rust logic testable with normal `cargo test`
* you avoid PyO3 features/env flags interfering with Rust-only tooling
* you can later add other frontends (CLI, WASM, etc.) without dragging Python concerns everywhere

### Pattern 3 (big-library pattern): “Python base package” + “runtime wheels”

**Polars** uses a sophisticated version of this: the *Python package* (`polars`) is built with setuptools and depends on separate **runtime** wheel packages (e.g., `polars-runtime-32`, optional `polars-runtime-64`, `polars-runtime-compat`). ([GitHub][4])

Why projects do this:

* large native codebases may need multiple runtime variants (CPU features, allocators, compatibility)
* you can keep the pure-Python layer (docs, typing, helpers) separate from native build concerns
* releases can validate that versions across the family match (Polars’ CI checks runtime versions against the Python package version). ([GitHub][5])

If you expect CPU-optimized vs compatibility builds, or multiple native components, this pattern is worth considering early.

---

## 3) Cargo + PyO3 configuration best practices

### 3.1 `#[pymodule]` name must match the compiled module name

PyO3 is blunt here: the module name should match `lib.name` in `Cargo.toml` so Python can import it without a custom loader. ([PyO3][6])

**Rule of thumb**

* If users do `import my_pkg._native`, then your compiled module name should be `_native`, and your Python package can re-export a friendlier API.

### 3.2 Prefer modern PyO3 linking behavior (avoid “dev pain”)

PyO3 explains that building extension modules should use `PYO3_BUILD_EXTENSION_MODULE` (set automatically by newer maturin / setuptools-rust), and that the older `extension-module` feature caused major problems because it disabled linking broadly (breaking tests/benches). ([PyO3][2])

**Practical guidance**

* If you’re starting fresh: rely on maturin to set the environment correctly.
* If you must keep `extension-module` for compatibility/history, isolate it to the bindings crate so it doesn’t poison your whole workspace.

(Polars’ runtime crate enables `pyo3`’s `extension-module` alongside `abi3-py39`, `chrono`, and `multiple-pymethods`. ([GitHub][7]))

### 3.3 Choosing abi3: do it deliberately, and document the tradeoff

PyO3’s guide lays out the motivation and the cost: fewer wheels, but fewer optimizations and some missing features depending on minimum Python version. ([PyO3][2])

**A sane policy**

* If your extension is mostly “thin glue” and you care about install simplicity: consider `abi3`.
* If your extension is performance-critical and benefits from CPython internals / version-specific optimizations: consider version-specific wheels.

Real-world note: Polars has discussed whether `abi3` is worth the performance tradeoff and what it means for their wheel strategy. ([GitHub][8])

### 3.4 Plan for free-threaded Python evolution

PyO3 exposes `#[pyo3(gil_used = true)]` on `#[pymodule]` to declare that the module requires the GIL to run safely under free-threaded Python builds. ([PyO3][6])
Python’s stable-ABI story for free-threaded builds is actively evolving (see ongoing discussion around stable ABI compatibility for free-threaded builds). ([Discussions on Python.org][9])

**Best practice**

* Keep GIL assumptions explicit in your code and docs
* Avoid undocumented reliance on CPython internals if you want long-term portability

---

## 4) `pyproject.toml` + maturin: clean packaging that tools understand

### 4.1 Always use PEP 517 `pyproject.toml`

The packaging guide strongly recommends a `[build-system]` table so build frontends know how to build your project. ([Python Packaging][10])
maturin documents the canonical setup: `build-backend = "maturin"` with a version range. ([Maturin][3])

### 4.2 Use `[tool.maturin]` to encode “how we build”

maturin supports configuration keys like:

* `profile` (Cargo profile for normal builds)
* `editable-profile` (use a faster dev profile for `maturin develop`)
* `features`, `all-features`, `no-default-features`, etc. ([Maturin][11])

**Practical setup idea**

* `editable-profile = "dev"` for fast local iteration
* `profile = "release"` (or a custom “dist” profile) for CI wheels

### 4.3 Set an explicit `module-name` for mixed packages

In the plugin ecosystem, you’ll often see a stable internal module path like `package._internal` or `package._lib`.

For example, Polars plugins commonly set `module-name` in `[tool.maturin]`:

* `polars-st` uses `module-name = "polars_st._lib"` and enables `pyo3/extension-module`. ([GitHub][12])
* `polars-pairing` uses `module-name = "polars_pairing._internal"`. ([GitHub][13])

This lets your Python code control the public API while the native module stays “private”.

---

## 5) Development workflow that stays pleasant

### 5.1 Fast local loop: `maturin develop`

Use `maturin develop` for editable installs in a venv. Pair it with maturin’s `editable-profile` setting so you aren’t compiling “full release” constantly. ([Maturin][11])

### 5.2 When builds act weird: inspect PyO3’s detected Python config

PyO3 documents:

* `PYO3_PYTHON` to force the interpreter used for build configuration
* `PYO3_PRINT_CONFIG=1` to print config and halt (excellent for debugging) ([PyO3][2])

### 5.3 Testing strategy

* Rust unit tests: test core logic in the core crate (no Python involved)
* Python tests: use `pytest` against the installed package
* Integration tests: keep them Python-side unless you truly need Rust-driven embedding

Avoid panics crossing the FFI boundary—treat panics as bugs, not control flow (panics create confusing Python exceptions and stack traces in user reports).

---

## 6) PyO3 binding patterns that produce clean Python APIs

### 6.1 Keep the boundary small and intentional

A good binding layer does:

* argument parsing / type conversion
* error translation
* resource ownership bridging
* releasing the GIL for heavy work

…and **does not** duplicate your business logic.

### 6.2 Module initialization: explicit exports

Prefer the specific add methods (`add_function`, `add_class`, `add_submodule`) rather than generic attribute setting—PyO3 recommends these for clarity and correctness. ([Mej's Map][14])

### 6.3 Don’t fight Python packaging—wrap your native module

Common pattern:

* native module: `my_pkg._native`
* Python `__init__.py` re-exports stable public functions/classes

This keeps import semantics stable even if you reorganize Rust code.

### 6.4 Error mapping: make Python errors feel native

Return `PyResult<T>` from all Python-exposed functions and convert Rust error types into appropriate Python exceptions (`ValueError`, `TypeError`, etc.). Users will thank you when tracebacks are actionable.

---

## 7) Performance patterns (where most projects leave speed on the table)

### 7.1 Release the GIL around heavy compute

If you do CPU-heavy work, don’t hold the GIL. Structure code so conversion happens quickly, then run compute in Rust, then convert results back.

### 7.2 Minimize copying with “native-friendly” data interchange

If your domain involves arrays / columnar data:

* design APIs that accept buffers/arrays in a form you can view without copying
* consider Arrow-based interchange when possible

In the Polars ecosystem, `pyo3-polars` provides wrappers like `PySeries` / `PyDataFrame` that implement `FromPyObject` / `IntoPy`, making conversion to/from Python easier and more standardized. ([Docs.rs][15])

### 7.3 Be careful with abi3 + advanced features

If you plan `abi3`, remember PyO3’s “missing features” list (buffer API availability, some class options, etc.). ([PyO3][2])
For string-heavy APIs, note that real projects track minimum Python versions partly because `abi3` limitations affect what conversions PyO3 can offer cleanly (Polars discusses minimum Python bumps with `abi3` considerations). ([GitHub][16])

---

## 8) Builds, wheels, CI: what “good” looks like

### 8.1 Use maturin for wheel builds (and automate it)

maturin’s own docs emphasize it as the “batteries included” way to build and publish PyO3-based packages. ([PyO3][17])

maturin also documents the standard CLI flow (`maturin build` producing wheels in `target/wheels`). ([GitHub][18])

### 8.2 CI matrix: follow established workflows

Polars’ release workflow is a strong reference:

* it builds an sdist for runtime crates with `PyO3/maturin-action@v1`
* it builds wheels across a large OS/arch matrix
* it uses custom `RUSTFLAGS` CPU features for different runtime variants (compat vs non-compat)
* it pins a specific maturin version in CI for repeatability ([GitHub][5])

Even if you don’t copy Polars’ complexity, the meta-lessons are:

* make build decisions explicit (profiles, features, env)
* validate version consistency automatically
* test the built wheels, not just the source tree

### 8.3 abi3 wheel correctness matters

If you publish `abi3` wheels, ensure the build configuration and wheel tags actually match what you built. Independent analysis has highlighted how easy it is for tooling to produce mismatched abi3 compatibility signals if you aren’t careful. ([The Trail of Bits Blog][19])

---

## 9) Suggested “starter template” (minimal, but scalable)

### Layout (mixed Python + Rust; recommended default)

```text
my_pkg/
  pyproject.toml
  Cargo.toml
  src/
    lib.rs                 # Rust extension module crate
  python/
    my_pkg/
      __init__.py          # Re-export public API
      _version.py
      py.typed             # if you ship typing markers
  tests/
    test_smoke.py
```

### `pyproject.toml` (maturin backend + explicit module name)

```toml
[build-system]
requires = ["maturin>=1.0,<2.0"]
build-backend = "maturin"

[project]
name = "my-pkg"
requires-python = ">=3.10"
# version can be static here, or dynamic via tooling (pick one policy and stick to it)

[tool.maturin]
module-name = "my_pkg._native"
# optional: use cargo profiles strategically
profile = "release"
editable-profile = "dev"
```

### `Cargo.toml` (bindings crate)

```toml
[package]
name = "my_pkg_native"
version = "0.1.0"
edition = "2021"

[lib]
name = "_native"
crate-type = ["cdylib"]

[dependencies]
pyo3 = { version = "0.28", features = ["abi3-py310"] } # or drop abi3 if you want version-specific wheels
```

### `src/lib.rs` (module definition)

```rust
use pyo3::prelude::*;

#[pyfunction]
fn add(a: i64, b: i64) -> i64 {
    a + b
}

#[pymodule]
fn _native(m: &Bound<'_, PyModule>) -> PyResult<()> {
    m.add_function(wrap_pyfunction!(add, m)?)?;
    Ok(())
}
```

Notes:

* Keep module naming consistent with `lib.name` / `module-name` to avoid import failures. ([PyO3][6])
* If you choose `abi3`, ensure you understand the feature limitations and test on multiple Python versions. ([PyO3][2])

---

## 10) “Ship it” checklist

* Naming: `#[pymodule]` name == `lib.name` (and `module-name` if you use it) ([PyO3][6])
* You have a mixed layout if you need non-Rust package data (stubs, `py.typed`, schemas, etc.) ([Maturin][1])
* CI builds wheels on all target platforms and **tests the wheels**
* If `abi3`: you’ve verified you’re not relying on missing features/optimizations ([PyO3][2])
* You can debug build detection quickly with `PYO3_PYTHON` / `PYO3_PRINT_CONFIG` ([PyO3][2])
* Heavy compute releases the GIL and avoids unnecessary copies
* Panic paths are treated as bugs (not part of the API)
</file>

<file path="bin/benchmark-remerge.sh">
#!/usr/bin/env bash
set -euo pipefail

BUILD_MODE="skip"
RUNS=1
ITERATIONS=5

while [[ $# -gt 0 ]]; do
  case "$1" in
    --build)
      BUILD_MODE="${2:-}"
      shift 2
      ;;
    --runs)
      RUNS="${2:-}"
      shift 2
      ;;
    --iterations)
      ITERATIONS="${2:-}"
      shift 2
      ;;
    -h|--help)
      cat <<'USAGE'
Usage: bin/benchmark-remerge.sh [--build skip|debug|release] [--runs N] [--iterations N]

Examples:
  bin/benchmark-remerge.sh --build release --runs 3 --iterations 5
  bin/benchmark-remerge.sh --build debug --runs 1 --iterations 2
  bin/benchmark-remerge.sh --build skip --runs 5 --iterations 5
USAGE
      exit 0
      ;;
    *)
      echo "Unknown argument: $1" >&2
      exit 1
      ;;
  esac
done

if [[ "$BUILD_MODE" != "skip" && "$BUILD_MODE" != "debug" && "$BUILD_MODE" != "release" ]]; then
  echo "--build must be one of: skip, debug, release" >&2
  exit 1
fi

if [[ "$BUILD_MODE" == "debug" ]]; then
  echo "[build] uv run --no-sync maturin develop"
  uv run --no-sync maturin develop
elif [[ "$BUILD_MODE" == "release" ]]; then
  echo "[build] uv run --no-sync maturin develop --release"
  uv run --no-sync maturin develop --release
fi

echo "[bench] runs=$RUNS iterations=$ITERATIONS"

uv run --no-sync python - <<PY
from pathlib import Path
from statistics import mean
from time import perf_counter

import remerge

runs = int(${RUNS})
iterations = int(${ITERATIONS})


def load_sample_corpus() -> list[list[str]]:
    corpus: list[list[str]] = []
    root = Path("tests/sample_corpus")
    for txt in sorted(root.glob("*.TXT")):
        for line in txt.read_text().split("\n"):
            if line:
                corpus.append(line.split(" "))
    return corpus

corpus = load_sample_corpus()

configs = [
    ("log_likelihood", {"method": "log_likelihood", "min_count": 0}),
    ("frequency", {"method": "frequency", "min_count": 0}),
    ("npmi", {"method": "npmi", "min_count": 25}),
]

for label, kwargs in configs:
    samples = []
    first_winner = None
    for _ in range(runs):
        t0 = perf_counter()
        winners = remerge.run(corpus, iterations=iterations, progress_bar="none", **kwargs)
        dt = perf_counter() - t0
        samples.append(dt)
        if first_winner is None and winners:
            first_winner = winners[0].merged_lexeme.word
    print(
        f"{label:14s} avg={mean(samples):8.3f}s min={min(samples):8.3f}s max={max(samples):8.3f}s first={first_winner}"
    )
PY
</file>

<file path="src/remerge/_core.pyi">
from typing import Optional

StepPayload = tuple[
    float,
    list[str],
    int,
    list[str],
    int,
    list[str],
    int,
    list[tuple[int, int]],
    list[tuple[int, int]],
]

StepOutcome = tuple[str, Optional[StepPayload], Optional[float]]


class Engine:
    def __init__(
        self,
        corpus: list[list[str]],
        method: str,
        min_count: int,
        tie_breaker: str,
    ) -> None: ...
    def corpus_length(self) -> int: ...
    def step(self, min_score: Optional[float] = None) -> StepOutcome: ...
</file>

<file path="tests/__init__.py">

</file>

<file path="tests/conftest.py">
from .fixtures import sample_corpus

__all__ = ["sample_corpus"]
</file>

<file path=".gitattributes">
tests/sample_corpus/* filter=lfs diff=lfs merge=lfs -text
</file>

<file path="Cargo.toml">
[package]
name = "remerge-rs"
version = "0.2.1"
edition = "2021"

[lib]
name = "_core"
path = "rust/src/lib.rs"
crate-type = ["cdylib", "rlib"]

[features]
default = []
python-extension = ["pyo3/extension-module"]

[dependencies]
indexmap = "2.12"
pyo3 = { version = "0.23.5", features = ["abi3-py312"] }
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2022 Peter Baumgartner

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="REMERGE_improvement_plan.md">
# REMERGE Improvement Plan (Balanced Reliability + Algorithm Quality)

## Summary
Stabilize correctness first (empty-candidate handling and `min_count` math), then add deterministic extraction controls and targeted algorithm quality improvements, while tightening tests and maintainability. Keep core behavior recognizable but make outcomes reproducible and safer for real corpora.

## Public API / Interface Changes
1. Extend `run()` with:
   - `tie_breaker: Literal["legacy_first_seen", "deterministic"] = "deterministic"`
   - `on_exhausted: Literal["stop", "raise"] = "stop"`
   - `min_score: Optional[float] = None` (early-stop threshold on winning score; method-specific interpretation)
2. Add `NoCandidateBigramError(ValueError)` used when `on_exhausted="raise"` and no valid candidate remains.
3. Replace `SelectionMethod` alias with `class SelectionMethod(str, Enum)` while continuing to accept equivalent string values for backward compatibility.

## Implementation Plan
1. Correctness foundation
   - Introduce a unified candidate-building path that returns scored candidates plus global totals from unfiltered bigram stats.
   - Apply `min_count` only as a candidate inclusion filter.
   - Ensure LL/NPMI denominator uses full current bigram total, not filtered total.
   - Make selectors return "no candidate" cleanly instead of throwing from `argmax`/indexing.
2. Run-loop robustness
   - In `run()`, handle "no candidate" via `on_exhausted`:
     - `"stop"`: break loop and return winners collected so far.
     - `"raise"`: raise `NoCandidateBigramError` with method/min_count context.
   - Apply `min_score` after selecting the best candidate; if below threshold, stop (or raise if `on_exhausted="raise"`).
3. Deterministic tie-breaking (algorithm quality + reproducibility)
   - For equal score, rank by:
     - higher raw bigram frequency
     - lexicographic order of concatenated token tuple (stable final tie-break)
   - Keep `"legacy_first_seen"` for compatibility testing and migration.
4. Maintainability/performance refactor
   - Compute `clean_bigram_locations` once per winner and reuse it through merge/update steps.
   - Change `bigrams_to_locations` value type from `List[...]` to `Set[...]` for O(1)-style removal and easier invariant checks.
   - Prune empty location entries immediately after updates.
   - Remove unused imports/types and isolate scoring math into small pure functions.
5. Test expansion
   - Add deterministic unit tests for:
     - empty corpus and single-token corpus
     - `iterations` greater than available merges
     - `min_count` filtering behavior across all methods (including frequency)
     - LL/NPMI denominator correctness with `min_count > 0`
     - tie resolution under both tie-breaker modes
     - `on_exhausted` stop vs raise behavior
     - `min_score` early stopping
   - Add invariant tests after each merge:
     - counter totals match location maps
     - no negative counts
     - winner bigram removed after merge
   - Sort fixture corpus file loading for deterministic test input order.
6. Docs and migration notes
   - Update README API table/examples for new `run()` options and deterministic default behavior.
   - Document semantic change: returned winners may be fewer than requested `iterations` when exhausted or thresholded.
   - Add short "reproducibility" section explaining tie-break choices.

## Test Cases and Scenarios
1. `run([["a"]], 5, on_exhausted="stop")` returns `[]` without error.
2. Same input with `on_exhausted="raise"` raises `NoCandidateBigramError`.
3. Frequency method respects `min_count` and excludes low-frequency winners.
4. NPMI/LL with `min_count` produce finite values and stable rankings using full denominators.
5. Tie corpus with shuffled line/file order gives identical output under `tie_breaker="deterministic"`.
6. Existing regression corpus still yields expected first winners unless explicitly changed by deterministic tie policy.
7. Merge invariants pass across multi-iteration synthetic corpora with repeated/overlapping patterns.

## Assumptions and Defaults
1. Small API additions are acceptable.
2. Balanced sequencing is preferred: each milestone includes one reliability item and one algorithm-quality item.
3. Default behavior should favor reproducibility and safety:
   - `tie_breaker="deterministic"`
   - `on_exhausted="stop"`
   - `min_score=None` (disabled unless user sets it)
4. Discontinuous/gapped bigrams remain out of scope for this iteration.
</file>

<file path="rust/src/lib.rs">
use indexmap::IndexMap;
use pyo3::exceptions::PyValueError;
use pyo3::prelude::*;
use std::cmp::Reverse;
use std::collections::{HashMap, HashSet};

const SMALL: f64 = 1e-10;
const SCORE_ATOL: f64 = 1e-12;
const SCORE_RTOL: f64 = 1e-12;

#[derive(Clone, Copy, Debug, Eq, PartialEq)]
enum SelectionMethod {
    Frequency,
    LogLikelihood,
    Npmi,
}

impl SelectionMethod {
    fn parse(value: &str) -> PyResult<Self> {
        match value {
            "frequency" => Ok(Self::Frequency),
            "log_likelihood" => Ok(Self::LogLikelihood),
            "npmi" => Ok(Self::Npmi),
            _ => Err(PyValueError::new_err(format!(
                "Invalid method {value:?}. Expected one of: 'frequency', 'log_likelihood', 'npmi'."
            ))),
        }
    }
}

#[derive(Clone, Copy, Debug, Eq, PartialEq)]
enum TieBreaker {
    Deterministic,
    LegacyFirstSeen,
}

impl TieBreaker {
    fn parse(value: &str) -> PyResult<Self> {
        match value {
            "deterministic" => Ok(Self::Deterministic),
            "legacy_first_seen" => Ok(Self::LegacyFirstSeen),
            _ => Err(PyValueError::new_err(format!(
                "Invalid tie_breaker {value:?}. Expected one of: 'deterministic', 'legacy_first_seen'."
            ))),
        }
    }
}

#[derive(Clone, Debug, Eq, PartialEq, Hash, Ord, PartialOrd)]
struct Lexeme {
    word: Vec<String>,
    ix: usize,
}

#[derive(Clone, Debug, Eq, PartialEq, Hash, Ord, PartialOrd)]
struct Bigram {
    left: Lexeme,
    right: Lexeme,
}

#[derive(Default)]
struct LexemeData {
    lexemes_to_locations: HashMap<Lexeme, HashSet<(usize, usize)>>,
    locations_to_lexemes: Vec<Vec<Lexeme>>,
    lexemes_to_freqs: HashMap<Lexeme, i64>,
}

impl LexemeData {
    fn from_corpus(corpus: &[Vec<String>]) -> Self {
        let mut lexeme_data = Self::default();
        for (line_ix, tokens) in corpus.iter().enumerate() {
            let mut line_lexemes = Vec::with_capacity(tokens.len());
            for (word_ix, word) in tokens.iter().enumerate() {
                let lexeme = Lexeme {
                    word: vec![word.clone()],
                    ix: 0,
                };
                let loc = (line_ix, word_ix);
                lexeme_data
                    .lexemes_to_locations
                    .entry(lexeme.clone())
                    .or_default()
                    .insert(loc);
                line_lexemes.push(lexeme);
            }
            lexeme_data.locations_to_lexemes.push(line_lexemes);
        }

        lexeme_data.lexemes_to_freqs = lexeme_data
            .lexemes_to_locations
            .iter()
            .filter(|(lexeme, _)| lexeme.ix == 0)
            .map(|(lexeme, locs)| (lexeme.clone(), locs.len() as i64))
            .collect();

        lexeme_data
    }

    fn corpus_length(&self) -> usize {
        self.locations_to_lexemes.len()
    }

    fn locations_to_root_lexemes(&self, line_ix: usize) -> Vec<(usize, Lexeme)> {
        self.locations_to_lexemes[line_ix]
            .iter()
            .enumerate()
            .filter(|(_, lexeme)| lexeme.ix == 0)
            .map(|(ix, lexeme)| (ix, lexeme.clone()))
            .collect()
    }
}

#[derive(Default)]
struct BigramData {
    bigrams_to_freqs: IndexMap<Bigram, i64>,
    bigrams_to_locations: HashMap<Bigram, HashSet<(usize, usize)>>,
    left_lex_freqs: HashMap<Lexeme, i64>,
    right_lex_freqs: HashMap<Lexeme, i64>,
}

impl BigramData {
    fn from_lexemes(lexeme_data: &LexemeData) -> Self {
        let mut bigram_data = Self::default();
        for line_ix in 0..lexeme_data.corpus_length() {
            let line_lexeme_data = lexeme_data.locations_to_root_lexemes(line_ix);
            let mut line_bigrams = Vec::new();
            for pair in line_lexeme_data.windows(2) {
                let left_ix = pair[0].0;
                let left = pair[0].1.clone();
                let right = pair[1].1.clone();
                let bigram = Bigram { left, right };
                let location = (line_ix, left_ix);
                bigram_data
                    .bigrams_to_locations
                    .entry(bigram.clone())
                    .or_default()
                    .insert(location);
                line_bigrams.push(bigram);
            }
            bigram_data.batch_add_bigrams(&line_bigrams);
        }
        bigram_data
    }

    fn batch_add_bigrams(&mut self, bigram_locations: &[Bigram]) {
        for bigram in bigram_locations {
            *self.left_lex_freqs.entry(bigram.left.clone()).or_insert(0) += 1;
            *self
                .right_lex_freqs
                .entry(bigram.right.clone())
                .or_insert(0) += 1;
            *self.bigrams_to_freqs.entry(bigram.clone()).or_insert(0) += 1;
        }
    }
}

#[derive(Clone)]
struct WinnerInfo {
    bigram: Bigram,
    merged_lexeme: Lexeme,
    bigram_locations: Vec<(usize, usize)>,
}

impl WinnerInfo {
    fn from_bigram_with_data(bigram: &Bigram, bigram_data: &BigramData) -> Self {
        let mut all_words = Vec::with_capacity(bigram.left.word.len() + bigram.right.word.len());
        all_words.extend_from_slice(&bigram.left.word);
        all_words.extend_from_slice(&bigram.right.word);

        let mut locations = bigram_data
            .bigrams_to_locations
            .get(bigram)
            .cloned()
            .unwrap_or_default()
            .into_iter()
            .collect::<Vec<_>>();
        locations.sort_unstable();

        Self {
            bigram: bigram.clone(),
            merged_lexeme: Lexeme {
                word: all_words,
                ix: 0,
            },
            bigram_locations: locations,
        }
    }

    fn n_lexemes(&self) -> usize {
        self.merged_lexeme.word.len()
    }

    fn cleaned_bigram_locations(&self) -> Vec<(usize, usize)> {
        let mut clean_locations = Vec::new();
        let mut ix = 0;
        while ix < self.bigram_locations.len() {
            let line = self.bigram_locations[ix].0;
            let mut token_ix = Vec::new();
            while ix < self.bigram_locations.len() && self.bigram_locations[ix].0 == line {
                token_ix.push(self.bigram_locations[ix].1);
                ix += 1;
            }

            let mut exclude_tokens: HashSet<usize> = HashSet::new();
            for token in token_ix.iter().copied() {
                if exclude_tokens.contains(&token) {
                    continue;
                }
                for candidate in token_ix.iter().copied() {
                    if token <= candidate && candidate < token + self.n_lexemes() {
                        exclude_tokens.insert(candidate);
                    }
                }
                clean_locations.push((line, token));
            }
        }

        clean_locations
    }
}

#[derive(Clone)]
struct ScoredBigram {
    bigram: Bigram,
    score: f64,
    frequency: i64,
}

impl ScoredBigram {
    fn merged_word(&self) -> Vec<String> {
        let mut merged =
            Vec::with_capacity(self.bigram.left.word.len() + self.bigram.right.word.len());
        merged.extend_from_slice(&self.bigram.left.word);
        merged.extend_from_slice(&self.bigram.right.word);
        merged
    }
}

struct BigramCandidateData {
    bigram_index: Vec<Bigram>,
    bigram_freq_array: Vec<i64>,
    el1_freq_array: Vec<i64>,
    el2_freq_array: Vec<i64>,
    total_bigram_count: i64,
}

impl BigramCandidateData {
    fn from_bigram_data(bigram_data: &BigramData, min_count: i64) -> Self {
        let total_bigram_count = bigram_data.bigrams_to_freqs.values().sum::<i64>();
        let candidate_items = bigram_data
            .bigrams_to_freqs
            .iter()
            .filter(|(_, freq)| **freq >= min_count)
            .collect::<Vec<_>>();

        if candidate_items.is_empty() {
            return Self {
                bigram_index: Vec::new(),
                bigram_freq_array: Vec::new(),
                el1_freq_array: Vec::new(),
                el2_freq_array: Vec::new(),
                total_bigram_count,
            };
        }

        let mut bigram_index = Vec::with_capacity(candidate_items.len());
        let mut bigram_freq_array = Vec::with_capacity(candidate_items.len());
        let mut el1_freq_array = Vec::with_capacity(candidate_items.len());
        let mut el2_freq_array = Vec::with_capacity(candidate_items.len());

        for (bigram, freq) in candidate_items {
            bigram_index.push((*bigram).clone());
            bigram_freq_array.push(*freq);
            el1_freq_array.push(*bigram_data.left_lex_freqs.get(&bigram.left).unwrap_or(&0));
            el2_freq_array.push(*bigram_data.right_lex_freqs.get(&bigram.right).unwrap_or(&0));
        }

        Self {
            bigram_index,
            bigram_freq_array,
            el1_freq_array,
            el2_freq_array,
            total_bigram_count,
        }
    }
}

fn safe_ll_term(observed: f64, expected: f64) -> f64 {
    if observed > 0.0 {
        observed * (((observed / (expected + SMALL)) + SMALL).ln())
    } else {
        0.0
    }
}

fn calculate_npmi(data: &BigramCandidateData) -> Vec<f64> {
    if data.total_bigram_count == 0 {
        return Vec::new();
    }

    let total = data.total_bigram_count as f64;
    let mut scores = Vec::with_capacity(data.bigram_freq_array.len());
    for i in 0..data.bigram_freq_array.len() {
        let prob_ab = data.bigram_freq_array[i] as f64 / total;
        let prob_a = data.el1_freq_array[i] as f64 / total;
        let prob_b = data.el2_freq_array[i] as f64 / total;

        let numerator = (prob_ab / (prob_a * prob_b)).ln();
        let denominator = -(prob_ab.ln());
        let npmi = if denominator > 0.0 {
            numerator / denominator
        } else {
            f64::NAN
        };

        let perfect_association =
            is_close_default(denominator, 0.0) && is_close_default(numerator, 0.0);
        if perfect_association {
            scores.push(1.0);
        } else {
            scores.push(npmi);
        }
    }

    scores
}

fn calculate_log_likelihood(data: &BigramCandidateData) -> Vec<f64> {
    if data.total_bigram_count == 0 {
        return Vec::new();
    }

    let total = data.total_bigram_count as f64;
    let mut scores = Vec::with_capacity(data.bigram_freq_array.len());

    for i in 0..data.bigram_freq_array.len() {
        let obs_a = data.bigram_freq_array[i] as f64;
        let obs_b = data.el1_freq_array[i] as f64 - obs_a;
        let obs_c = data.el2_freq_array[i] as f64 - obs_a;
        let mut obs_d = total - obs_a - obs_b - obs_c;
        if obs_d < 0.0 {
            obs_d = 0.0;
        }

        let exp_a = ((obs_a + obs_b) * (obs_a + obs_c)) / total;
        let exp_b = ((obs_a + obs_b) * (obs_b + obs_d)) / total;
        let exp_c = ((obs_c + obs_d) * (obs_a + obs_c)) / total;
        let exp_d = ((obs_c + obs_d) * (obs_b + obs_d)) / total;

        let ll_a = safe_ll_term(obs_a, exp_a);
        let ll_b = safe_ll_term(obs_b, exp_b);
        let ll_c = safe_ll_term(obs_c, exp_c);
        let ll_d = safe_ll_term(obs_d, exp_d);

        let log_likelihood = 2.0 * (ll_a + ll_b + ll_c + ll_d);
        if obs_a > exp_a {
            scores.push(log_likelihood);
        } else {
            scores.push(-log_likelihood);
        }
    }

    scores
}

fn coerce_scores(scores: Vec<f64>) -> Vec<f64> {
    scores
        .into_iter()
        .map(|score| {
            if score.is_finite() {
                score
            } else {
                f64::NEG_INFINITY
            }
        })
        .collect()
}

fn as_scored_bigrams(data: &BigramCandidateData, scores: Vec<f64>) -> Vec<ScoredBigram> {
    let safe_scores = coerce_scores(scores);
    if safe_scores.is_empty() || safe_scores.iter().all(|score| *score == f64::NEG_INFINITY) {
        return Vec::new();
    }

    data.bigram_index
        .iter()
        .zip(safe_scores.iter())
        .zip(data.bigram_freq_array.iter())
        .map(|((bigram, score), freq)| ScoredBigram {
            bigram: bigram.clone(),
            score: *score,
            frequency: *freq,
        })
        .collect()
}

fn calculate_candidates_log_likelihood(
    bigram_data: &BigramData,
    min_count: i64,
) -> Vec<ScoredBigram> {
    let data = BigramCandidateData::from_bigram_data(bigram_data, min_count);
    let scores = calculate_log_likelihood(&data);
    as_scored_bigrams(&data, scores)
}

fn calculate_candidates_npmi(bigram_data: &BigramData, min_count: i64) -> Vec<ScoredBigram> {
    let data = BigramCandidateData::from_bigram_data(bigram_data, min_count);
    let scores = calculate_npmi(&data);
    as_scored_bigrams(&data, scores)
}

fn calculate_candidates_frequency(bigram_data: &BigramData, min_count: i64) -> Vec<ScoredBigram> {
    bigram_data
        .bigrams_to_freqs
        .iter()
        .filter(|(_, freq)| **freq >= min_count)
        .map(|(bigram, freq)| ScoredBigram {
            bigram: bigram.clone(),
            score: *freq as f64,
            frequency: *freq,
        })
        .collect()
}

fn scores_close(a: f64, b: f64) -> bool {
    (a - b).abs() <= (SCORE_ATOL + SCORE_RTOL * b.abs())
}

fn is_close_default(a: f64, b: f64) -> bool {
    (a - b).abs() <= 1e-8 + (1e-5 * b.abs())
}

fn select_candidate(candidates: &[ScoredBigram], tie_breaker: TieBreaker) -> Option<ScoredBigram> {
    if candidates.is_empty() {
        return None;
    }

    let max_score = candidates
        .iter()
        .map(|candidate| candidate.score)
        .fold(f64::NEG_INFINITY, f64::max);

    if tie_breaker == TieBreaker::LegacyFirstSeen {
        return candidates
            .iter()
            .find(|candidate| scores_close(candidate.score, max_score))
            .cloned();
    }

    let mut best_candidate: Option<ScoredBigram> = None;
    let mut best_key: Option<(Reverse<i64>, Vec<String>)> = None;

    for candidate in candidates {
        if !scores_close(candidate.score, max_score) {
            continue;
        }
        let candidate_key = (Reverse(candidate.frequency), candidate.merged_word());
        match (&best_candidate, &best_key) {
            (None, None) => {
                best_candidate = Some(candidate.clone());
                best_key = Some(candidate_key);
            }
            (Some(_), Some(key)) => {
                if candidate_key < *key {
                    best_candidate = Some(candidate.clone());
                    best_key = Some(candidate_key);
                }
            }
            _ => {}
        }
    }

    best_candidate
}

fn adjacent_bigrams(lexemes: &[Lexeme]) -> Vec<Bigram> {
    lexemes
        .windows(2)
        .map(|pair| Bigram {
            left: pair[0].clone(),
            right: pair[1].clone(),
        })
        .collect()
}

fn merge_winner(
    winner: &WinnerInfo,
    clean_locations: &[(usize, usize)],
    lexeme_data: &mut LexemeData,
    bigram_data: &mut BigramData,
) {
    let bigram_lines = clean_locations
        .iter()
        .map(|(line_ix, _)| *line_ix)
        .collect::<HashSet<_>>();

    let mut touched_lexemes = HashSet::new();
    touched_lexemes.insert(winner.merged_lexeme.clone());
    touched_lexemes.insert(winner.bigram.left.clone());
    touched_lexemes.insert(winner.bigram.right.clone());

    let mut touched_bigrams = HashSet::new();
    touched_bigrams.insert(winner.bigram.clone());

    let mut old_bigrams_lookup: HashMap<usize, Vec<(usize, Lexeme)>> = HashMap::new();
    for line_ix in bigram_lines {
        old_bigrams_lookup.insert(line_ix, lexeme_data.locations_to_root_lexemes(line_ix));
    }

    for (line_ix, word_ix) in clean_locations.iter().copied() {
        for lexeme_index in 0..winner.n_lexemes() {
            let pos = word_ix + lexeme_index;
            let old_lexeme = lexeme_data.locations_to_lexemes[line_ix][pos].clone();
            touched_lexemes.insert(old_lexeme.clone());

            let lexeme = Lexeme {
                word: winner.merged_lexeme.word.clone(),
                ix: lexeme_index,
            };
            lexeme_data.locations_to_lexemes[line_ix][pos] = lexeme.clone();

            if let Some(locations) = lexeme_data.lexemes_to_locations.get_mut(&old_lexeme) {
                locations.remove(&(line_ix, pos));
            }
            lexeme_data
                .lexemes_to_locations
                .entry(lexeme)
                .or_default()
                .insert((line_ix, pos));
        }
    }

    for (line_ix, old_root_items) in old_bigrams_lookup {
        let old_root_lexemes = old_root_items
            .iter()
            .map(|(_, lexeme)| lexeme.clone())
            .collect::<Vec<_>>();
        let old_bigrams = adjacent_bigrams(&old_root_lexemes);
        for bigram in old_bigrams.iter().cloned() {
            touched_bigrams.insert(bigram);
        }

        let new_root_items = lexeme_data.locations_to_root_lexemes(line_ix);
        let new_root_lexemes = new_root_items
            .iter()
            .map(|(_, lexeme)| lexeme.clone())
            .collect::<Vec<_>>();
        let new_bigrams = adjacent_bigrams(&new_root_lexemes);
        for bigram in new_bigrams.iter().cloned() {
            touched_bigrams.insert(bigram);
        }

        for bigram in &new_bigrams {
            *bigram_data
                .bigrams_to_freqs
                .entry(bigram.clone())
                .or_insert(0) += 1;
            *bigram_data
                .left_lex_freqs
                .entry(bigram.left.clone())
                .or_insert(0) += 1;
            *bigram_data
                .right_lex_freqs
                .entry(bigram.right.clone())
                .or_insert(0) += 1;
        }

        for bigram in &old_bigrams {
            *bigram_data
                .bigrams_to_freqs
                .entry(bigram.clone())
                .or_insert(0) -= 1;
            *bigram_data
                .left_lex_freqs
                .entry(bigram.left.clone())
                .or_insert(0) -= 1;
            *bigram_data
                .right_lex_freqs
                .entry(bigram.right.clone())
                .or_insert(0) -= 1;
        }

        for pair in old_root_items.windows(2) {
            let left_ix = pair[0].0;
            let left = pair[0].1.clone();
            let right = pair[1].1.clone();
            let bigram = Bigram { left, right };
            let location = (line_ix, left_ix);
            if let Some(locations) = bigram_data.bigrams_to_locations.get_mut(&bigram) {
                locations.remove(&location);
            }
        }

        for pair in new_root_items.windows(2) {
            let left_ix = pair[0].0;
            let left = pair[0].1.clone();
            let right = pair[1].1.clone();
            let bigram = Bigram { left, right };
            let location = (line_ix, left_ix);
            bigram_data
                .bigrams_to_locations
                .entry(bigram)
                .or_default()
                .insert(location);
        }
    }

    let merge_token_count = clean_locations.len() as i64;
    lexeme_data
        .lexemes_to_freqs
        .insert(winner.merged_lexeme.clone(), merge_token_count);

    if let Some(el1_freq) = lexeme_data.lexemes_to_freqs.get_mut(&winner.bigram.left) {
        *el1_freq -= merge_token_count;
    }
    if let Some(el2_freq) = lexeme_data.lexemes_to_freqs.get_mut(&winner.bigram.right) {
        *el2_freq -= merge_token_count;
    }

    for lexeme in touched_lexemes {
        let remove_freq = lexeme_data
            .lexemes_to_freqs
            .get(&lexeme)
            .map(|freq| *freq <= 0)
            .unwrap_or(false);
        if remove_freq {
            lexeme_data.lexemes_to_freqs.remove(&lexeme);
        }

        let remove_locations = lexeme_data
            .lexemes_to_locations
            .get(&lexeme)
            .map(|locations| locations.is_empty())
            .unwrap_or(true);
        if remove_locations {
            lexeme_data.lexemes_to_locations.remove(&lexeme);
        }
    }

    let mut touched_lr_lexemes = HashSet::new();
    for bigram in touched_bigrams {
        touched_lr_lexemes.insert(bigram.left.clone());
        touched_lr_lexemes.insert(bigram.right.clone());

        let remove_freq = bigram_data
            .bigrams_to_freqs
            .get(&bigram)
            .map(|freq| *freq <= 0)
            .unwrap_or(false);
        if remove_freq {
            bigram_data.bigrams_to_freqs.shift_remove(&bigram);
        }

        let remove_locations = bigram_data
            .bigrams_to_locations
            .get(&bigram)
            .map(|locations| locations.is_empty())
            .unwrap_or(true);
        if remove_locations {
            bigram_data.bigrams_to_locations.remove(&bigram);
        }
    }

    for lexeme in touched_lr_lexemes {
        let remove_left = bigram_data
            .left_lex_freqs
            .get(&lexeme)
            .map(|freq| *freq <= 0)
            .unwrap_or(false);
        if remove_left {
            bigram_data.left_lex_freqs.remove(&lexeme);
        }

        let remove_right = bigram_data
            .right_lex_freqs
            .get(&lexeme)
            .map(|freq| *freq <= 0)
            .unwrap_or(false);
        if remove_right {
            bigram_data.right_lex_freqs.remove(&lexeme);
        }
    }

    debug_assert!(!bigram_data.bigrams_to_freqs.contains_key(&winner.bigram));
}

#[derive(Clone)]
struct StepData {
    score: f64,
    winner: WinnerInfo,
    clean_locations: Vec<(usize, usize)>,
}

enum StepStatus {
    Winner(StepData),
    NoCandidate,
    BelowMinScore(f64),
}

#[pyclass]
struct Engine {
    lexemes: LexemeData,
    bigrams: BigramData,
    method: SelectionMethod,
    min_count: i64,
    tie_breaker: TieBreaker,
}

impl Engine {
    fn calculate_candidates(&self) -> Vec<ScoredBigram> {
        match self.method {
            SelectionMethod::Frequency => {
                calculate_candidates_frequency(&self.bigrams, self.min_count)
            }
            SelectionMethod::LogLikelihood => {
                calculate_candidates_log_likelihood(&self.bigrams, self.min_count)
            }
            SelectionMethod::Npmi => calculate_candidates_npmi(&self.bigrams, self.min_count),
        }
    }

    fn step_internal(&mut self, min_score: Option<f64>) -> StepStatus {
        let candidates = self.calculate_candidates();
        let Some(selected) = select_candidate(&candidates, self.tie_breaker) else {
            return StepStatus::NoCandidate;
        };

        if let Some(score_threshold) = min_score {
            if selected.score < score_threshold {
                return StepStatus::BelowMinScore(selected.score);
            }
        }

        let winner = WinnerInfo::from_bigram_with_data(&selected.bigram, &self.bigrams);
        let clean_locations = winner.cleaned_bigram_locations();
        merge_winner(
            &winner,
            &clean_locations,
            &mut self.lexemes,
            &mut self.bigrams,
        );

        StepStatus::Winner(StepData {
            score: selected.score,
            winner,
            clean_locations,
        })
    }
}

type StepPayload = (
    f64,
    Vec<String>,
    usize,
    Vec<String>,
    usize,
    Vec<String>,
    usize,
    Vec<(usize, usize)>,
    Vec<(usize, usize)>,
);

type StepOutcome = (String, Option<StepPayload>, Option<f64>);

#[pymethods]
impl Engine {
    #[new]
    fn new(
        corpus: Vec<Vec<String>>,
        method: &str,
        min_count: usize,
        tie_breaker: &str,
    ) -> PyResult<Self> {
        let method = SelectionMethod::parse(method)?;
        let tie_breaker = TieBreaker::parse(tie_breaker)?;
        let lexemes = LexemeData::from_corpus(&corpus);
        let bigrams = BigramData::from_lexemes(&lexemes);

        Ok(Self {
            lexemes,
            bigrams,
            method,
            min_count: min_count as i64,
            tie_breaker,
        })
    }

    fn corpus_length(&self) -> usize {
        self.lexemes.corpus_length()
    }

    #[pyo3(signature = (min_score=None))]
    fn step(&mut self, py: Python<'_>, min_score: Option<f64>) -> StepOutcome {
        let step_result = py.allow_threads(|| self.step_internal(min_score));
        match step_result {
            StepStatus::NoCandidate => ("no_candidate".to_string(), None, None),
            StepStatus::BelowMinScore(score) => ("below_min_score".to_string(), None, Some(score)),
            StepStatus::Winner(step_data) => {
                let payload = (
                    step_data.score,
                    step_data.winner.bigram.left.word,
                    step_data.winner.bigram.left.ix,
                    step_data.winner.bigram.right.word,
                    step_data.winner.bigram.right.ix,
                    step_data.winner.merged_lexeme.word,
                    step_data.winner.merged_lexeme.ix,
                    step_data.winner.bigram_locations,
                    step_data.clean_locations,
                );
                ("winner".to_string(), Some(payload), Some(step_data.score))
            }
        }
    }
}

// This module currently assumes GIL use for Python interaction.
#[pymodule(gil_used = true)]
fn _core(_py: Python<'_>, module: &Bound<'_, PyModule>) -> PyResult<()> {
    module.add_class::<Engine>()?;
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    fn build_engine(corpus: Vec<Vec<&str>>, method: SelectionMethod, min_count: i64) -> Engine {
        let corpus = corpus
            .into_iter()
            .map(|line| line.into_iter().map(str::to_string).collect::<Vec<_>>())
            .collect::<Vec<_>>();

        let lexemes = LexemeData::from_corpus(&corpus);
        let bigrams = BigramData::from_lexemes(&lexemes);
        Engine {
            lexemes,
            bigrams,
            method,
            min_count,
            tie_breaker: TieBreaker::Deterministic,
        }
    }

    #[test]
    fn deterministic_tie_break_prefers_frequency_then_lexicographic() {
        let mut engine = build_engine(
            vec![
                vec!["a", "d"],
                vec!["a", "c"],
                vec!["a", "b"],
                vec!["a", "b"],
            ],
            SelectionMethod::Frequency,
            0,
        );
        let StepStatus::Winner(step) = engine.step_internal(None) else {
            panic!("expected winner");
        };
        assert_eq!(step.winner.merged_lexeme.word, vec!["a", "b"]);
    }

    #[test]
    fn legacy_tie_break_is_first_seen() {
        let corpus = vec![vec!["c", "d"], vec!["a", "b"]]
            .into_iter()
            .map(|line| line.into_iter().map(str::to_string).collect::<Vec<_>>())
            .collect::<Vec<_>>();
        let lexemes = LexemeData::from_corpus(&corpus);
        let bigrams = BigramData::from_lexemes(&lexemes);
        let mut engine = Engine {
            lexemes,
            bigrams,
            method: SelectionMethod::Frequency,
            min_count: 0,
            tie_breaker: TieBreaker::LegacyFirstSeen,
        };

        let StepStatus::Winner(step) = engine.step_internal(None) else {
            panic!("expected winner");
        };
        assert_eq!(step.winner.merged_lexeme.word, vec!["c", "d"]);
    }

    #[test]
    fn min_score_blocks_low_score_winner() {
        let mut engine = build_engine(vec![vec!["a", "b", "c"]], SelectionMethod::Frequency, 0);
        let status = engine.step_internal(Some(10.0));
        let StepStatus::BelowMinScore(score) = status else {
            panic!("expected below-min-score status");
        };
        assert_eq!(score, 1.0);
    }

    #[test]
    fn merge_invariants_hold_on_small_corpus() {
        let mut engine = build_engine(
            vec![
                vec!["a", "a", "a", "a"],
                vec!["c", "a", "b", "a", "b", "a", "b", "d"],
            ],
            SelectionMethod::Frequency,
            0,
        );

        for _ in 0..8 {
            let status = engine.step_internal(None);
            let StepStatus::Winner(step) = status else {
                break;
            };
            assert!(!engine
                .bigrams
                .bigrams_to_freqs
                .contains_key(&step.winner.bigram));
            assert!(engine
                .lexemes
                .lexemes_to_freqs
                .values()
                .all(|freq| *freq > 0));
            assert!(engine
                .bigrams
                .bigrams_to_freqs
                .values()
                .all(|freq| *freq > 0));
            assert!(engine
                .lexemes
                .lexemes_to_locations
                .values()
                .all(|locations| !locations.is_empty()));
            assert!(engine
                .bigrams
                .bigrams_to_locations
                .values()
                .all(|locations| !locations.is_empty()));
        }
    }

    #[test]
    fn frequency_respects_min_count() {
        let mut engine = build_engine(
            vec![vec!["a", "b"], vec!["a", "c"]],
            SelectionMethod::Frequency,
            2,
        );
        let status = engine.step_internal(None);
        assert!(matches!(status, StepStatus::NoCandidate));
    }

    #[test]
    fn empty_or_single_token_corpus_has_no_candidate() {
        let mut empty_engine = build_engine(vec![], SelectionMethod::Frequency, 0);
        assert!(matches!(
            empty_engine.step_internal(None),
            StepStatus::NoCandidate
        ));

        let mut single_engine = build_engine(vec![vec!["a"]], SelectionMethod::Frequency, 0);
        assert!(matches!(
            single_engine.step_internal(None),
            StepStatus::NoCandidate
        ));
    }

    #[test]
    fn consecutive_merge_path_is_greedy() {
        let mut engine = build_engine(
            vec![vec!["a", "a", "a", "a"]],
            SelectionMethod::Frequency,
            0,
        );

        let StepStatus::Winner(first) = engine.step_internal(None) else {
            panic!("expected first winner");
        };
        assert_eq!(first.clean_locations.len(), 2);

        let StepStatus::Winner(second) = engine.step_internal(None) else {
            panic!("expected second winner");
        };
        assert_eq!(second.winner.merged_lexeme.word, vec!["a", "a", "a", "a"]);
    }

    #[test]
    fn scoring_methods_select_winners_on_small_corpus() {
        let corpus = vec![vec!["a", "b", "a", "b", "c"], vec!["a", "b", "d", "e"]];
        for method in [
            SelectionMethod::Frequency,
            SelectionMethod::LogLikelihood,
            SelectionMethod::Npmi,
        ] {
            let mut engine = build_engine(corpus.clone(), method, 0);
            assert!(matches!(engine.step_internal(None), StepStatus::Winner(_)));
        }
    }
}
</file>

<file path="tests/test_smoke.py">
import pytest

from remerge import run


@pytest.mark.fast
def test_minimal_run_smoke() -> None:
    winners = run([["a", "b", "a", "b"]], 1, method="frequency", progress_bar="none")
    assert winners
    assert winners[0].merged_lexeme.word == ("a", "b")
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

scripts/
*.json
</file>

<file path="CITATION.cff">
# This CITATION.cff file was generated with cffinit.
# Visit https://bit.ly/cffinit to generate yours today!

cff-version: 1.2.0
title: REMERGE - Multi-Word Expression discovery algorithm
message: >-
  If you use this software, please cite it using the
  metadata from this file.
type: software
authors:
  - given-names: ' Peter'
    family-names: Baumgartner
identifiers:
  - type: url
    value: 'https://doi.org/10.5281/zenodo.7130548'
    description: >-
      GitHub repository for REMERGE - Multi-Word
      Expression discovery algorithm
repository-code: 'https://github.com/pmbaumgartner/remerge-mwe'
abstract: >-
  REMERGE is a Multi-Word Expression (MWE) discovery
  algorithm, which started as a re-implementation and
  simplification of a similar algorithm called MERGE,
  detailed in a publication and Ph.D. thesis[1,2].
  The primary benefit of this algorithm is that it's
  non-parametric regarding the size of the n-grams
  that constitute an MWE—you do not need to specify a
  priori how many n-grams comprise an MWE—you only
  need to specify the number of iterations you want
  the algorithm to run.

  [1] A. Wahl and S. Th. Gries, “Multi-word
  Expressions: A Novel Computational Approach to
  Their Bottom-Up Statistical Extraction,” in Lexical
  Collocation Analysis, P. Cantos-Gómez and M.
  Almela-Sánchez, Eds. Cham: Springer International
  Publishing, 2018, pp. 85–109. doi:
  10.1007/978-3-319-92582-0_5.

  [2] A. Wahl, “The Distributional Learning of
  Multi-Word Expressions: A Computational Approach,”
  p. 190.
keywords:
  - multiword expressions
  - corpus linguistics
  - computational linguistics
license: MIT
commit: 6b1a713f156cb0d5abff6866f1889596fd04a3ad
version: 0.1.1
date-released: '2022-09-30'
</file>

<file path="tests/fixtures.py">
from pathlib import Path

import pytest


@pytest.fixture(scope="session")
def sample_corpus():
    """We'll use the corpus included in the original implementation,
    which is "[...] a combination of the Santa Barbara
    Corpus of Spoken American English and the spoken
    component of the ICE Canada corpus"

    > Du Bois, John W., Wallace L. Chafe, Charles Meyers, Sandra A. Thompson, Nii Martey, and Robert Englebretson (2005). Santa Barbara corpus of spoken American English. Philadelphia: Linguistic Data Consortium.

    > Newman, John and Georgie Columbus (2010). The International Corpus of English – Canada. Edmonton, Alberta: University of Alberta.

    Note that these are dialogue corpuses, so each line is often referred to as a `turn`. They've also been pre-processed with lowercasing, and punctuation replaced with alphanumeric substitutions (`_` --> `undrscr`).
    """
    corpus: list[list[str]] = []
    this_folder = Path(__file__).parent
    txt_files = sorted((this_folder / Path("sample_corpus/")).glob("*.TXT"))
    for txt_file in txt_files:
        for line in txt_file.read_text().split("\n"):
            if line:
                tokens: list[str] = line.split(" ")
                corpus.append(tokens)
    return corpus
</file>

<file path="AGENTS.md">
# AGENTS.md

## Project Tooling
- Package and environment management: `uv`
- Linting/formatting: `ruff`
- Type checking: `ty`

## Standard Workflow
1. Sync the environment after pulling or changing dependencies:
   - `uv sync --all-groups`
2. If dependency constraints changed in `pyproject.toml`, refresh lock + sync:
   - `uv lock`
   - `uv sync --all-groups --frozen`
3. After code changes, run quality checks (formatting first):
   - `uv run ruff format src tests`
   - `uv run ruff check src tests`
   - `uv run ty check src tests`
4. Run tests before finishing:
   - `uv run --no-sync pytest -v -m "not corpus and not parity"`

## Rust/PyO3 Fast Loop
1. Recommended command order for quickest feedback:
   - `cargo check`
   - `cargo test`
   - `uv run --no-sync maturin develop`
   - `uv run --no-sync pytest -q tests/test_smoke.py`
   - `uv run --no-sync pytest -q -m "fast"`
2. Build/install the Rust extension after Rust code changes:
   - `uv run --no-sync maturin develop`
3. Run targeted tests while iterating:
   - `uv run --no-sync pytest -q -m "fast" tests/test_remerge.py -k "<pattern>"`
4. If behavior looks stale, rebuild the extension first:
   - `uv run --no-sync maturin develop`
5. If PyO3 build detection seems wrong, print config and verify interpreter:
   - `PYO3_PRINT_CONFIG=1 uv run --no-sync maturin develop`
   - `PYO3_PYTHON=.venv/bin/python uv run --no-sync maturin develop`
6. If `cargo test` fails with unresolved Python symbols, verify `Cargo.toml` is not forcing `pyo3/extension-module` and rerun:
   - `cargo clean`
   - `cargo test`

## Full Verification Before Handoff
1. Python checks:
   - `uv run ruff format src tests`
   - `uv run ruff check src tests`
   - `uv run ty check src tests`
   - `uv run --no-sync pytest -v -m "not corpus and not parity"`
   - `uv run --no-sync pytest -v -m "corpus or parity"`
2. Rust checks:
   - `cargo fmt --all`
   - `cargo clippy --all-targets -- -D warnings`
   - `cargo test`
</file>

<file path=".github/workflows/tests.yml">
on:
  push:
    branches: [main]
  pull_request:

jobs:
  fast-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - uses: dtolnay/rust-toolchain@stable
      - uses: astral-sh/setup-uv@v4
      - name: cache uv
        uses: actions/cache@v4
        with:
          path: ~/.cache/uv
          key: uv-${{ runner.os }}-py312-${{ hashFiles('**/uv.lock') }}
      - run: uv sync --all-groups --frozen
      - run: cargo check
      - run: cargo test
      - run: cargo clippy --all-targets -- -D warnings
      - run: uv run --with maturin maturin develop
      - run: uv run --no-sync pytest -q tests/test_smoke.py
      - run: uv run --with maturin maturin build --release -o dist
      - run: |
          python -m venv .wheel-smoke
          . .wheel-smoke/bin/activate
          pip install --upgrade pip
          pip install dist/remerge_mwe-*.whl
          python -c "import remerge; assert callable(remerge.run); assert remerge.run([['a','b']], 1, method='frequency', progress_bar='none')[0].merged_lexeme.word == ('a', 'b')"
      - run: uv run --no-sync pytest -v -m "not corpus and not parity"

  full-corpus:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: fast-test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - uses: dtolnay/rust-toolchain@stable
      - uses: astral-sh/setup-uv@v4
      - name: cache uv
        uses: actions/cache@v4
        with:
          path: ~/.cache/uv
          key: uv-${{ runner.os }}-py312-${{ hashFiles('**/uv.lock') }}
      - run: uv sync --all-groups --frozen
      - run: uv run --with maturin maturin develop
      - run: uv run --no-sync pytest -v -m "corpus or parity"
</file>

<file path="src/remerge/__init__.py">
from .core import (
    ExhaustionPolicy,
    NoCandidateBigramError,
    SelectionMethod,
    TieBreaker,
    run,
)

__version__ = "0.2.1"

__all__ = [
    "run",
    "SelectionMethod",
    "TieBreaker",
    "ExhaustionPolicy",
    "NoCandidateBigramError",
]
</file>

<file path="pyproject.toml">
[project]
name = "remerge-mwe"
version = "0.2.1"
description = "REMERGE is a Multi-Word Expression (MWE) discovery algorithm derived from the MERGE algorithm."
authors = [{name = "Peter Baumgartner", email = "5107405+pmbaumgartner@users.noreply.github.com"}]
readme = "README.md"
license = "MIT"
requires-python = ">=3.12"
dependencies = [
    "tqdm>=4.64.1,<5",
    "numpy>=2,<3",
]

[dependency-groups]
dev = [
    "pytest>=7.1.3,<8",
    "ipython>=8.5.0,<9",
    "ruff>=0.9,<1",
    "ty>=0.0.1a16",
    "py-spy>=0.3.14,<0.4",
    "pytest-sugar>=0.9.5,<0.10",
    "pytest-cov>=4.0.0,<5",
]

[tool.maturin]
python-source = "src"
module-name = "remerge._core"
features = ["python-extension"]
profile = "release"
editable-profile = "dev"

[build-system]
requires = ["maturin>=1.8,<2.0"]
build-backend = "maturin"

[tool.pytest.ini_options]
markers = [
    "fast: quick, small-corpus tests for local iteration",
    "corpus: tests that exercise the bundled sample corpus",
    "parity: full-corpus regression parity checks (slow)",
]
</file>

<file path="tests/test_remerge.py">
import json
from pathlib import Path

import pytest
from remerge import __version__, run
from remerge.core import Lexeme, NoCandidateBigramError, SelectionMethod


def _summarize_winners(winners):
    return [
        {
            "merged_word": list(winner.merged_lexeme.word),
            "merge_token_count": winner.merge_token_count,
        }
        for winner in winners
    ]


def test_version():
    assert __version__ == "0.2.1"


@pytest.mark.corpus
def test_sample_corpus_single_iter(sample_corpus):
    winners = run(sample_corpus, 1, progress_bar="none")
    assert winners[0].merged_lexeme == Lexeme(("you", "know"), 0)


@pytest.mark.fast
def test_winner_shape():
    winners = run([["a", "b", "c"]], 1, method="frequency", progress_bar="none")
    winner = winners[0]
    assert winner.bigram[0] == Lexeme(("a",), 0)
    assert winner.bigram[1] == Lexeme(("b",), 0)
    assert winner.merged_lexeme == Lexeme(("a", "b"), 0)
    assert winner.n_lexemes == 2
    assert winner.merge_token_count == 1


@pytest.mark.fast
def test_consecutive_single():
    """Ensure greedy bigram merge avoids overlapping middle bigram."""
    corpus = [["a", "a", "a", "a"]]
    winners = run(corpus, 2, progress_bar="none")
    assert winners[0].merge_token_count == 2
    assert winners[1].merged_lexeme == Lexeme(("a", "a", "a", "a"), 0)


@pytest.mark.fast
def test_consecutive_remainder():
    """Ensure greedy bigram merge avoids overlapping trailing bigram."""
    corpus = [["c", "a", "b", "a", "b", "a", "b", "d"]]
    winners = run(corpus, 2, method="frequency", progress_bar="none")
    assert winners[0].merge_token_count == 3
    assert winners[1].merge_token_count == 1


@pytest.mark.fast
@pytest.mark.parametrize("progress_bar", ["all", "iterations", "none"])
def test_progress_bar(progress_bar):
    corpus = [["a", "b", "a", "b", "c"], ["a", "b", "d", "e"]]
    winners = run(corpus, 2, progress_bar=progress_bar)
    assert len(winners) == 2


@pytest.mark.fast
@pytest.mark.parametrize(
    "method,min_count",
    [
        (SelectionMethod.log_likelihood, 0),
        (SelectionMethod.npmi, 0),
        (SelectionMethod.frequency, 0),
        ("frequency", 0),
    ],
)
def test_methods(method, min_count):
    corpus = [["a", "b", "a", "b", "c"], ["a", "b", "d", "e"]]
    winners = run(corpus, 2, method=method, min_count=min_count, progress_bar="none")
    assert len(winners) == 2


@pytest.mark.fast
def test_output(tmp_path):
    output = tmp_path / "tmp.json"
    winners = run([["a", "b", "a", "b"]], 1, output=output, progress_bar="none")
    results = json.loads(output.read_text())
    assert tuple(results["0"]) == winners[0].merged_lexeme.word


@pytest.mark.fast
def test_output_writes_each_iteration_with_cumulative_content(monkeypatch, tmp_path):
    output = tmp_path / "tmp.json"
    corpus = [["a", "a", "a", "a"]]
    writes = []
    original_write_text = Path.write_text

    def _capture_write(path_obj, text, *args, **kwargs):
        if path_obj == output:
            writes.append(text)
        return original_write_text(path_obj, text, *args, **kwargs)

    monkeypatch.setattr(Path, "write_text", _capture_write)
    winners = run(corpus, 2, output=output, progress_bar="none")

    assert len(writes) == len(winners)
    for i, payload in enumerate(writes):
        decoded = json.loads(payload)
        expected = {
            str(j): list(w.merged_lexeme.word) for j, w in enumerate(winners[: i + 1])
        }
        assert decoded == expected
    assert json.loads(output.read_text()) == json.loads(writes[-1])


@pytest.mark.fast
def test_empty_or_single_token_corpus_stops_cleanly():
    assert run([], 1, progress_bar="none") == []
    assert run([["a"]], 5, progress_bar="none") == []


@pytest.mark.fast
def test_exhausted_policy_raise():
    with pytest.raises(NoCandidateBigramError):
        run([["a"]], 1, on_exhausted="raise", progress_bar="none")


@pytest.mark.fast
def test_iterations_larger_than_available_merges_stop():
    winners = run([["a", "b", "c"]], 99, progress_bar="none")
    assert len(winners) == 2


@pytest.mark.fast
def test_frequency_respects_min_count():
    corpus = [["a", "b"], ["a", "c"]]
    winners = run(
        corpus,
        1,
        method="frequency",
        min_count=2,
        on_exhausted="stop",
        progress_bar="none",
    )
    assert winners == []


@pytest.mark.fast
@pytest.mark.parametrize("method", ["frequency", "log_likelihood", "npmi"])
def test_deterministic_tie_breaking_is_order_independent(method):
    corpus_a = [["a", "b"], ["c", "d"]]
    corpus_b = [["c", "d"], ["a", "b"]]

    winner_a = run(corpus_a, 1, method=method, progress_bar="none")[
        0
    ].merged_lexeme.word
    winner_b = run(corpus_b, 1, method=method, progress_bar="none")[
        0
    ].merged_lexeme.word
    assert winner_a == winner_b == ("a", "b")


@pytest.mark.fast
def test_legacy_tie_breaking_keeps_first_seen_behavior():
    corpus_a = [["a", "b"], ["c", "d"]]
    corpus_b = [["c", "d"], ["a", "b"]]

    winner_a = run(
        corpus_a,
        1,
        method="frequency",
        tie_breaker="legacy_first_seen",
        progress_bar="none",
    )[0].merged_lexeme.word
    winner_b = run(
        corpus_b,
        1,
        method="frequency",
        tie_breaker="legacy_first_seen",
        progress_bar="none",
    )[0].merged_lexeme.word

    assert winner_a != winner_b


@pytest.mark.fast
def test_min_score_stops_or_raises():
    corpus = [["a", "b", "c"]]
    assert run(corpus, 2, min_score=1e9, on_exhausted="stop", progress_bar="none") == []

    with pytest.raises(NoCandidateBigramError):
        run(corpus, 2, min_score=1e9, on_exhausted="raise", progress_bar="none")


@pytest.mark.parity
def test_parity_fixture(sample_corpus):
    parity = json.loads(Path("tests/parity_expected.json").read_text())

    ll = run(
        sample_corpus, 5, method="log_likelihood", min_count=0, progress_bar="none"
    )
    freq = run(sample_corpus, 5, method="frequency", min_count=0, progress_bar="none")
    npmi = run(sample_corpus, 5, method="npmi", min_count=25, progress_bar="none")

    assert _summarize_winners(ll) == parity["log_likelihood"]
    assert _summarize_winners(freq) == parity["frequency"]
    assert _summarize_winners(npmi) == parity["npmi"]

    tie_corpus = [["a", "b"], ["c", "d"]]
    for method, expected in parity["tie_deterministic"].items():
        winner = run(tie_corpus, 1, method=method, progress_bar="none")[0]
        assert list(winner.merged_lexeme.word) == expected
</file>

<file path="src/remerge/core.py">
import json
from dataclasses import dataclass
from enum import Enum
from functools import cached_property
from itertools import groupby
from pathlib import Path
from typing import Literal, NewType, TypeVar

from tqdm import tqdm, trange

from ._core import Engine


class SelectionMethod(str, Enum):
    frequency = "frequency"
    log_likelihood = "log_likelihood"
    npmi = "npmi"


class TieBreaker(str, Enum):
    deterministic = "deterministic"
    legacy_first_seen = "legacy_first_seen"


class ExhaustionPolicy(str, Enum):
    stop = "stop"
    raise_ = "raise"


class NoCandidateBigramError(ValueError):
    """Raised when no candidate bigrams are available for selection."""


@dataclass(frozen=True, slots=True)
class Lexeme:
    word: tuple[str, ...]
    ix: int

    def __repr__(self) -> str:
        return f"({self.word}|{self.ix})"


LineIndex = NewType("LineIndex", int)
TokenIndex = NewType("TokenIndex", int)
Bigram = tuple[Lexeme, Lexeme]
ProgressBarOptions = Literal["all", "iterations", "none"]


@dataclass(frozen=True)
class WinnerInfo:
    bigram: Bigram
    merged_lexeme: Lexeme
    bigram_locations: list[tuple[LineIndex, TokenIndex]]

    @cached_property
    def cleaned_bigram_locations(self) -> tuple[tuple[LineIndex, TokenIndex], ...]:
        """Greedily select non-overlapping bigram starts per line."""
        clean_locations: list[tuple[LineIndex, TokenIndex]] = []
        for line, location_group in groupby(self.bigram_locations, key=lambda x: x[0]):
            exclude_tokens: set[TokenIndex] = set()
            token_ix = [i[1] for i in location_group]
            for token in token_ix:
                if token in exclude_tokens:
                    continue
                excludes = [i for i in token_ix if token <= i < token + self.n_lexemes]
                exclude_tokens.update(excludes)
                clean_locations.append((line, token))
        return tuple(clean_locations)

    def clean_bigram_locations(self) -> list[tuple[LineIndex, TokenIndex]]:
        return list(self.cleaned_bigram_locations)

    @property
    def n_lexemes(self) -> int:
        return len(self.merged_lexeme.word)

    @property
    def merge_token_count(self) -> int:
        return len(self.cleaned_bigram_locations)


StepPayload = tuple[
    float,
    list[str],
    int,
    list[str],
    int,
    list[str],
    int,
    list[tuple[int, int]],
    list[tuple[int, int]],
]


EnumType = TypeVar("EnumType", bound=Enum)


def _coerce_enum(
    value: EnumType | str, enum_type: type[EnumType], argument_name: str
) -> EnumType:
    if isinstance(value, enum_type):
        return value
    try:
        return enum_type(value)
    except ValueError as exc:
        options = ", ".join(repr(option.value) for option in enum_type)
        raise ValueError(
            f"Invalid {argument_name} {value!r}. Expected one of: {options}."
        ) from exc


def _winner_from_payload(
    payload: StepPayload,
) -> tuple[WinnerInfo, float, set[LineIndex]]:
    (
        selected_score,
        left_word,
        left_ix,
        right_word,
        right_ix,
        merged_word,
        merged_ix,
        bigram_locations,
        cleaned_locations,
    ) = payload

    winner = WinnerInfo(
        bigram=(
            Lexeme(tuple(left_word), left_ix),
            Lexeme(tuple(right_word), right_ix),
        ),
        merged_lexeme=Lexeme(tuple(merged_word), merged_ix),
        bigram_locations=[
            (LineIndex(line_ix), TokenIndex(token_ix))
            for (line_ix, token_ix) in bigram_locations
        ],
    )
    line_hits = {LineIndex(line_ix) for (line_ix, _) in cleaned_locations}
    return winner, selected_score, line_hits


def run(
    corpus: list[list[str]],
    iterations: int,
    *,
    method: SelectionMethod | str = SelectionMethod.log_likelihood,
    min_count: int = 0,
    output: Path | None = None,
    progress_bar: ProgressBarOptions = "iterations",
    tie_breaker: TieBreaker | str = TieBreaker.deterministic,
    on_exhausted: ExhaustionPolicy | str = ExhaustionPolicy.stop,
    min_score: float | None = None,
) -> list[WinnerInfo]:
    """Run the remerge algorithm."""
    method = _coerce_enum(method, SelectionMethod, "method")
    tie_breaker = _coerce_enum(tie_breaker, TieBreaker, "tie_breaker")
    on_exhausted = _coerce_enum(on_exhausted, ExhaustionPolicy, "on_exhausted")

    winners: list[WinnerInfo] = []
    engine = Engine(corpus, method.value, min_count, tie_breaker.value)

    if output is not None:
        print(f"Outputting winning merged lexemes to '{output}'")

    iterations_iter = (
        trange(iterations)
        if progress_bar in {"all", "iterations"}
        else range(iterations)
    )

    for _ in iterations_iter:
        status, payload, selected_score = engine.step(min_score)

        if status == "no_candidate":
            if on_exhausted is ExhaustionPolicy.raise_:
                raise NoCandidateBigramError(
                    f"No candidate bigrams available for method={method.value!r} "
                    f"and min_count={min_count}."
                )
            break

        if status == "below_min_score":
            if on_exhausted is ExhaustionPolicy.raise_:
                raise NoCandidateBigramError(
                    f"Best candidate score ({selected_score}) is below "
                    f"min_score ({min_score})."
                )
            break

        if status != "winner" or payload is None:
            raise RuntimeError(f"Unexpected engine status {status!r}.")

        winner, score, line_hits = _winner_from_payload(payload)
        winners.append(winner)

        if output is not None:
            winner_lexemes = {i: w.merged_lexeme.word for i, w in enumerate(winners)}
            output.write_text(json.dumps(winner_lexemes))

        if isinstance(iterations_iter, tqdm):
            corpus_length = engine.corpus_length()
            pct_bgr = len(line_hits) / corpus_length if corpus_length else 0.0
            iterations_iter.set_postfix(
                {
                    "last_winner": winner.merged_lexeme.word,
                    "score": f"{score:.4g}",
                    "pct_bgr": f"{pct_bgr * 100:.1f}%",
                }
            )

    return winners
</file>

<file path="README.md">
# REMERGE - Multi-Word Expression discovery algorithm

REMERGE is a Multi-Word Expression (MWE) discovery algorithm, which started as a re-implementation and simplification of a similar algorithm called MERGE, detailed in a publication and PhD thesis[^2][^3]. The primary benefit of this algorithm is that it's non-parametric in regards to the size of the n-grams that constitute a MWE—you do not need to specify a priori how many n-grams comprise a MWE—you only need to specify the number of iterations you want the algorithm to run.

The code was originally derived from an existing implementation from the original author[^1] that I reviewed, converted from python 2 to 3, then modified and updated with the following:
- a correction of the log-likelihood calculation; previously it was not using the correct values for the contingency table
- the removal of gapsize / discontinuous bigrams (see below for issues with the prior implementation)
- an overall reduction in codebase size and complexity
  - ~60% reduction in loc
  - removed `pandas` and `nltk` dependencies
- type annotations
- the inclusion of additional metrics (Frequency, [NPMI](https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf)[^4]) for selecting the winning bigram.
- corrections for merging sequential bigrams greedily and completely.
  - e.g. `'ya ya ya ya'` -> `'(ya ya) (ya ya)'` -> `'(ya ya ya ya)'`. Previously the merge order was non-deterministic, and you could end up with `'ya (ya ya) ya'`
- An overall simplification of the algorithm. 
  - As a tradeoff, this version may be less efficient. After a bigram is merged into a single lexeme in the original implementation, new bigrams and conflicting (old) bigrams were respectively added and subtracted from a mutable counter of bigrams. The counts of this object were difficult to track and validate, and resulted in errors in certain cases, so I removed this step. Instead, only the lexeme data is updated with the new merged lexemes. Then, we track which lines contain the merged lexeme and create an *update* counter that subtracts the old bigrams from the new bigrams and updates the bigram data using that counter.
- [Clarified license with the original author](https://github.com/awahl1/MERGE/commit/0a118df852a573fa6db4cc9aea00fd8c691b52fa) and licensed as MIT.

#### Usage

```python
import remerge

corpus = [
    ["a", "list", "of", "already", "tokenized", "texts"],
    ["where", "each", "item", "is", "a", "list", "of", "tokens"],
    ["isn't", "a", "list", "nice"]
]

winners = remerge.run(
    corpus, iterations=1, method=remerge.SelectionMethod.frequency, progress_bar="all"
)
# winners[0].merged_lexeme.word == ('a', 'list')
```

There are 3 bigram winner selection methods: [Log-Likelihood (G²)](https://aclanthology.org/J93-1003.pdf)[^5], [NPMI](https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf)[^4], and raw frequency. They are available under the `SelectionMethod` enum. The default is log-likelihood, which was used in the original implementation.

If using `NPMI` (`SelectionMethod.npmi`), you likely want to provide a `min_count` parameter, "as infrequent word pairs tend to dominate the top of bigramme lists that are ranked after PMI". (p. 4[^4])

```python
winners = remerge.run(corpus, 100, method=remerge.SelectionMethod.npmi, min_count=25)
```

#### API - `remerge.run`

| Argument     | Type                           | Description                                                                                                                                                                                                                                                                                     |
| ------------ | ------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| corpus       | `List[List[str]]`              | A corpus of already tokenized texts.                                                                                                                                                                                                                                                            |
| iterations   | `int`                          | The maximum number of iterations to run the algorithm. Papers typically use >500.                                                                                                                                                                                                               |
| method       | `SelectionMethod`, optional    | One of "frequency", "log_likelihood", or "npmi". Defaults to "log_likelihood".                                                                                                                                                                                                                  |
| min_count    | `int`, optional                | The minimum count required for a bigram to be included in winner calculations for all methods. If choosing NPMI ("npmi"), prefer using min_count because this measure is biased towards infrequent word pairs. Defaults to 0.                                                               |
| output       | `Optional[Path]`, optional     | A file path to output the winning merged lexemes as JSON. Defaults to None.                                                                                                                                                                                                                     |
| progress_bar | `ProgressBarOptions`, optional | Verbosity of progress bar. "all" will display the lexeme and bigram construction progress each iteration plus total iteration progress. "iterations" will display progress on total iterations. "none" has no output. Defaults to "iterations".                                             |
| tie_breaker  | `TieBreaker`, optional         | How ties are resolved among equal-scoring candidates. "deterministic" ranks by score, then frequency, then lexicographic merged token order. "legacy_first_seen" uses prior first-seen behavior. Defaults to "deterministic".                                                             |
| on_exhausted | `ExhaustionPolicy`, optional   | Behavior when no candidate passes filters (or threshold): "stop" returns winners collected so far, "raise" raises `NoCandidateBigramError`. Defaults to "stop".                                                                                                                              |
| min_score    | `Optional[float]`, optional    | Optional minimum score threshold for the selected winner. If the best candidate is below this threshold, behavior follows `on_exhausted`. Defaults to None.                                                                                                                                   |

#### Install

Latest release:

```bash
pip install -U remerge-mwe
```

For latest from github:

```bash
pip install git+https://github.com/pmbaumgartner/remerge-mwe.git 
```

#### Development

Use [`uv`](https://github.com/astral-sh/uv) for local project and dependency management.
This package now builds a Rust extension via PyO3 + maturin, so a local Rust toolchain is required.

Create/sync the environment with all dependency groups:

```bash
uv sync --all-groups
```

Build/install the extension for the current environment:

```bash
uv run --no-sync maturin develop
```

Run tests:

```bash
uv run --no-sync pytest -v -m "not corpus and not parity"
```

Run full corpus/parity checks (slower, intended for CI/mainline validation):

```bash
uv run --no-sync pytest -v -m "corpus or parity"
```

Add a runtime dependency:

```bash
uv add <pkg>
```

Add a development dependency:

```bash
uv add --dev <pkg>
```

If you make changes under `rust/`, run `uv run --no-sync maturin develop` again before testing.

PyO3 troubleshooting:

```bash
# Print PyO3 interpreter/build config and stop.
PYO3_PRINT_CONFIG=1 uv run --no-sync maturin develop

# Force the Python interpreter PyO3 should inspect.
PYO3_PYTHON=.venv/bin/python uv run --no-sync maturin develop
```

If `cargo test` fails with unresolved Python symbols, confirm `pyo3/extension-module`
is not forced in `Cargo.toml`, then run:

```bash
cargo clean
cargo test
```

#### Rust/PyO3 Backlog

Planned follow-up after stabilization: split into a pure Rust core crate plus a thin
PyO3 bindings crate.

Trigger for this split:
1. Parity tests remain stable across 2 consecutive PRs.
2. CI remains green across 2 consecutive PRs.
3. Benchmark baseline from `bin/benchmark-remerge.sh --build release --runs 1 --iterations 5` is stable across 2 consecutive PRs.

#### How it works

The algorithm operates iteratively in two stages: first, it collects all bigrams of co-occurring `lexemes` in the corpus. A measure is calculated on the set of all bigrams to determine a winner. The two lexemes that comprise the winning bigram are merged into a single lexeme. Instances of that bigram (`lexeme` pair) in the corpus are replaced with the merged lexeme. Outdated bigrams, i.e. those that don't exist anymore because one of their elements is now a merged lexeme, are subtracted from the bigram data. New bigrams, i.e. those where one element is now a merged lexeme, are added to the bigram data. With this new set of bigram data, the process repeats and a new winner is selected.

At initialization, a `lexeme` consists of only a single token, but as the algorithm iterates lexemes become multi-word expressions formed from the winning bigrams. `Lexemes` contain two parts: a `word` which is a tuple of strings, and an `index` which represents the position of that specific token in a MWE. For example, if the winning bigram is `(you, know)`, occurrences of that sequence of lexemes will be replaced with `[(you, know), 0]` and `[(you, know), 1]` in the corpus. When bigrams are counted, only a root lexeme (where the index is 0) can form a bigram, so merged tokens don't get double counted. For a more visual explanation of a few iterations assuming specific winners, see the image below.

<img src="explanation.png" alt="An explanation of the remerge algorithm" width="820">

#### Limitations

This implementation is still a greedy agglomerative procedure, so local winner choices can influence later merges. Different selection methods (`frequency`, `log_likelihood`, `npmi`) can lead to materially different MWE inventories depending on corpus size and domain.

#### Issues with Original Algorithm

##### Single Bigrams with discontinuities forming from distinct Lexeme positions

One issue with discontinuities or gaps in the original algorithm is that it did not distinguish the position of a satellite lexeme occurring to the left or right of a bigram with a gap.

Take for example these two example sentences, using `-` to represent an arbitrary token:

```
a b c -
a - c b
```

Assume in a prior iteration, a winning bigram was `(a, _, c)`, representing the token `a`, a gap of `1`, and then the token `c`. with a gapsize of 1. The past algorithm, run on the above corpus, would count the token `b` twice towards the same n-gram `(a, b, c)`, despite there being two distinct n-grams represented here: `(a, b, c)` and `(a, _, c, b)`.

I think the algorithm is counting on the fact that it would be very rare to encounter this sequence of lexemes in a realistic corpus, where the same word would appear within the gap **and** after the gap. I think this is more of an artifact of this specific example with an unrealistically small vocabulary.

#### References

[^1]: awahl1, MERGE. 2017. Accessed: Jul. 11, 2022. [Online]. Available: https://github.com/awahl1/MERGE

[^2]: A. Wahl and S. Th. Gries, “Multi-word Expressions: A Novel Computational Approach to Their Bottom-Up Statistical Extraction,” in Lexical Collocation Analysis, P. Cantos-Gómez and M. Almela-Sánchez, Eds. Cham: Springer International Publishing, 2018, pp. 85–109. doi: 10.1007/978-3-319-92582-0_5.

[^3]: A. Wahl, “The Distributional Learning of Multi-Word Expressions: A Computational Approach,” p. 190.

[^4]: G. Bouma, “Normalized (Pointwise) Mutual Information in Collocation Extraction,” p. 11.

[^5]: T. Dunning, “Accurate Methods for the Statistics of Surprise and Coincidence,” Computational Linguistics, vol. 19, no. 1, p. 14.
</file>

</files>
