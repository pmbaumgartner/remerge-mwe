This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: docs/**, **/*.TXT
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    tests.yml
bin/
  benchmark-remerge.sh
rust/
  src/
    lib.rs
src/
  remerge/
    __init__.py
    _core.pyi
    core.py
tests/
  __init__.py
  conftest.py
  fixtures.py
  test_remerge.py
  test_smoke.py
.gitattributes
.gitignore
AGENTS.md
Cargo.toml
CITATION.cff
explanation.png
LICENSE
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="bin/benchmark-remerge.sh">
#!/usr/bin/env bash
set -euo pipefail

BUILD_MODE="skip"
RUNS=1
ITERATIONS=5

while [[ $# -gt 0 ]]; do
  case "$1" in
    --build)
      BUILD_MODE="${2:-}"
      shift 2
      ;;
    --runs)
      RUNS="${2:-}"
      shift 2
      ;;
    --iterations)
      ITERATIONS="${2:-}"
      shift 2
      ;;
    -h|--help)
      cat <<'USAGE'
Usage: bin/benchmark-remerge.sh [--build skip|debug|release] [--runs N] [--iterations N]

Examples:
  bin/benchmark-remerge.sh --build release --runs 3 --iterations 5
  bin/benchmark-remerge.sh --build debug --runs 1 --iterations 2
  bin/benchmark-remerge.sh --build skip --runs 5 --iterations 5
USAGE
      exit 0
      ;;
    *)
      echo "Unknown argument: $1" >&2
      exit 1
      ;;
  esac
done

if [[ "$BUILD_MODE" != "skip" && "$BUILD_MODE" != "debug" && "$BUILD_MODE" != "release" ]]; then
  echo "--build must be one of: skip, debug, release" >&2
  exit 1
fi

if [[ "$BUILD_MODE" == "debug" ]]; then
  echo "[build] uv run --no-sync maturin develop"
  uv run --no-sync maturin develop
elif [[ "$BUILD_MODE" == "release" ]]; then
  echo "[build] uv run --no-sync maturin develop --release"
  uv run --no-sync maturin develop --release
fi

echo "[bench] runs=$RUNS iterations=$ITERATIONS"

uv run --no-sync python - <<PY
from pathlib import Path
from statistics import mean
from time import perf_counter

import remerge

runs = int(${RUNS})
iterations = int(${ITERATIONS})


def load_sample_corpus() -> list[list[str]]:
    corpus: list[list[str]] = []
    root = Path("tests/sample_corpus")
    for txt in sorted(root.glob("*.TXT")):
        for line in txt.read_text().split("\n"):
            if line:
                corpus.append(line.split(" "))
    return corpus

corpus = load_sample_corpus()

configs = [
    ("log_likelihood", {"method": "log_likelihood", "min_count": 0}),
    ("frequency", {"method": "frequency", "min_count": 0}),
    ("npmi", {"method": "npmi", "min_count": 25}),
]

for label, kwargs in configs:
    samples = []
    first_winner = None
    for _ in range(runs):
        t0 = perf_counter()
        winners = remerge.run(corpus, iterations=iterations, progress_bar="none", **kwargs)
        dt = perf_counter() - t0
        samples.append(dt)
        if first_winner is None and winners:
            first_winner = winners[0].merged_lexeme.word
    print(
        f"{label:14s} avg={mean(samples):8.3f}s min={min(samples):8.3f}s max={max(samples):8.3f}s first={first_winner}"
    )
PY
</file>

<file path="tests/__init__.py">

</file>

<file path="tests/conftest.py">
from .fixtures import sample_corpus

__all__ = ["sample_corpus"]
</file>

<file path=".gitattributes">
tests/sample_corpus/* filter=lfs diff=lfs merge=lfs -text
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2022 Peter Baumgartner

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="src/remerge/_core.pyi">
from typing import Optional

StepPayload = tuple[
    float,
    list[str],
    int,
    list[str],
    int,
    list[str],
    int,
    list[tuple[int, int]],
]

ProgressPayload = tuple[int, float, list[str]]
RunOutcome = tuple[str, list[StepPayload], Optional[float], int, list[ProgressPayload]]

class Engine:
    def __init__(
        self,
        corpus: list[list[str]],
        method: str,
        min_count: int,
        tie_breaker: str,
    ) -> None: ...
    def corpus_length(self) -> int: ...
    def run(
        self,
        iterations: int,
        min_score: Optional[float] = None,
        return_progress: bool = False,
    ) -> RunOutcome: ...
</file>

<file path="tests/test_smoke.py">
import pytest

from remerge import run


@pytest.mark.fast
def test_minimal_run_smoke() -> None:
    winners = run([["a", "b", "a", "b"]], 1, method="frequency", progress_bar="none")
    assert winners
    assert winners[0].merged_lexeme.word == ("a", "b")
</file>

<file path="CITATION.cff">
# This CITATION.cff file was generated with cffinit.
# Visit https://bit.ly/cffinit to generate yours today!

cff-version: 1.2.0
title: REMERGE - Multi-Word Expression discovery algorithm
message: >-
  If you use this software, please cite it using the
  metadata from this file.
type: software
authors:
  - given-names: ' Peter'
    family-names: Baumgartner
identifiers:
  - type: url
    value: 'https://doi.org/10.5281/zenodo.7130548'
    description: >-
      GitHub repository for REMERGE - Multi-Word
      Expression discovery algorithm
repository-code: 'https://github.com/pmbaumgartner/remerge-mwe'
abstract: >-
  REMERGE is a Multi-Word Expression (MWE) discovery
  algorithm, which started as a re-implementation and
  simplification of a similar algorithm called MERGE,
  detailed in a publication and Ph.D. thesis[1,2].
  The primary benefit of this algorithm is that it's
  non-parametric regarding the size of the n-grams
  that constitute an MWE—you do not need to specify a
  priori how many n-grams comprise an MWE—you only
  need to specify the number of iterations you want
  the algorithm to run.

  [1] A. Wahl and S. Th. Gries, “Multi-word
  Expressions: A Novel Computational Approach to
  Their Bottom-Up Statistical Extraction,” in Lexical
  Collocation Analysis, P. Cantos-Gómez and M.
  Almela-Sánchez, Eds. Cham: Springer International
  Publishing, 2018, pp. 85–109. doi:
  10.1007/978-3-319-92582-0_5.

  [2] A. Wahl, “The Distributional Learning of
  Multi-Word Expressions: A Computational Approach,”
  p. 190.
keywords:
  - multiword expressions
  - corpus linguistics
  - computational linguistics
license: MIT
commit: 6b1a713f156cb0d5abff6866f1889596fd04a3ad
version: 0.1.1
date-released: '2022-09-30'
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

scripts/
*.json
.docs/
</file>

<file path="Cargo.toml">
[package]
name = "remerge-rs"
version = "0.2.1"
edition = "2021"

[lib]
name = "_core"
path = "rust/src/lib.rs"
crate-type = ["cdylib", "rlib"]

[features]
default = []
python-extension = ["pyo3/extension-module"]

[dependencies]
indexmap = "2.12"
pyo3 = { version = "0.23.5", features = ["abi3-py312"] }
rayon = "1.10"
smallvec = "1.13"
</file>

<file path="rust/src/lib.rs">
use pyo3::exceptions::PyValueError;
use pyo3::prelude::*;
use rayon::prelude::*;
use smallvec::{smallvec, SmallVec};
use std::cmp::Reverse;
use std::collections::{HashMap, HashSet};

const SMALL: f64 = 1e-10;
const SCORE_ATOL: f64 = 1e-12;
const SCORE_RTOL: f64 = 1e-12;
const FULL_RESCORE_INTERVAL: usize = 25;

type TokenId = u32;
type Location = (usize, usize);

#[derive(Clone, Copy, Debug, Eq, PartialEq)]
enum SelectionMethod {
    Frequency,
    LogLikelihood,
    Npmi,
}

impl SelectionMethod {
    fn parse(value: &str) -> PyResult<Self> {
        match value {
            "frequency" => Ok(Self::Frequency),
            "log_likelihood" => Ok(Self::LogLikelihood),
            "npmi" => Ok(Self::Npmi),
            _ => Err(PyValueError::new_err(format!(
                "Invalid method {value:?}. Expected one of: 'frequency', 'log_likelihood', 'npmi'."
            ))),
        }
    }
}

#[derive(Clone, Copy, Debug, Eq, PartialEq)]
enum TieBreaker {
    Deterministic,
    LegacyFirstSeen,
}

impl TieBreaker {
    fn parse(value: &str) -> PyResult<Self> {
        match value {
            "deterministic" => Ok(Self::Deterministic),
            "legacy_first_seen" => Ok(Self::LegacyFirstSeen),
            _ => Err(PyValueError::new_err(format!(
                "Invalid tie_breaker {value:?}. Expected one of: 'deterministic', 'legacy_first_seen'."
            ))),
        }
    }
}

#[derive(Default)]
struct Interner {
    str_to_id: HashMap<String, TokenId>,
    id_to_str: Vec<String>,
}

impl Interner {
    fn from_corpus(corpus: &[Vec<String>]) -> Self {
        let mut uniq = HashSet::new();
        for line in corpus {
            for token in line {
                uniq.insert(token.clone());
            }
        }
        let mut sorted = uniq.into_iter().collect::<Vec<_>>();
        sorted.sort_unstable();

        let mut interner = Self::default();
        for token in sorted {
            let id = interner.id_to_str.len() as TokenId;
            interner.str_to_id.insert(token.clone(), id);
            interner.id_to_str.push(token);
        }
        interner
    }

    fn id_for(&self, value: &str) -> TokenId {
        *self
            .str_to_id
            .get(value)
            .expect("token missing in interner while converting corpus")
    }

    fn ids_to_strings(&self, ids: &[TokenId]) -> Vec<String> {
        ids.iter()
            .map(|id| self.id_to_str[*id as usize].clone())
            .collect()
    }
}

#[derive(Clone, Debug, Eq, PartialEq, Hash, Ord, PartialOrd)]
struct Lexeme {
    word: SmallVec<[TokenId; 3]>,
    ix: usize,
}

#[derive(Clone, Debug, Eq, PartialEq, Hash, Ord, PartialOrd)]
struct Bigram {
    left: Lexeme,
    right: Lexeme,
}

#[derive(Default)]
struct LexemeData {
    lexemes_to_locations: HashMap<Lexeme, HashSet<Location>>,
    locations_to_lexemes: Vec<Vec<Lexeme>>,
    lexemes_to_freqs: HashMap<Lexeme, i64>,
}

impl LexemeData {
    fn from_corpus(corpus: &[Vec<TokenId>]) -> Self {
        let mut lexeme_data = Self::default();
        for (line_ix, tokens) in corpus.iter().enumerate() {
            let mut line_lexemes = Vec::with_capacity(tokens.len());
            for (word_ix, word) in tokens.iter().enumerate() {
                let lexeme = Lexeme {
                    word: smallvec![*word],
                    ix: 0,
                };
                let loc = (line_ix, word_ix);
                lexeme_data
                    .lexemes_to_locations
                    .entry(lexeme.clone())
                    .or_default()
                    .insert(loc);
                line_lexemes.push(lexeme);
            }
            lexeme_data.locations_to_lexemes.push(line_lexemes);
        }

        lexeme_data.lexemes_to_freqs = lexeme_data
            .lexemes_to_locations
            .iter()
            .filter(|(lexeme, _)| lexeme.ix == 0)
            .map(|(lexeme, locs)| (lexeme.clone(), locs.len() as i64))
            .collect();

        lexeme_data
    }

    fn corpus_length(&self) -> usize {
        self.locations_to_lexemes.len()
    }

    fn root_items_for_line(&self, line_ix: usize) -> Vec<(usize, Lexeme)> {
        self.locations_to_lexemes[line_ix]
            .iter()
            .enumerate()
            .filter(|(_, lexeme)| lexeme.ix == 0)
            .map(|(ix, lexeme)| (ix, lexeme.clone()))
            .collect()
    }
}

#[derive(Default)]
struct BigramData {
    bigrams_to_freqs: HashMap<Bigram, i64>,
    bigrams_first_seen_order: HashMap<Bigram, usize>,
    next_bigram_order: usize,
    bigrams_to_locations: HashMap<Bigram, HashSet<Location>>,
    left_lex_freqs: HashMap<Lexeme, i64>,
    right_lex_freqs: HashMap<Lexeme, i64>,
}

impl BigramData {
    fn from_lexemes(lexeme_data: &LexemeData) -> Self {
        let mut bigram_data = Self::default();
        for line_ix in 0..lexeme_data.corpus_length() {
            let root_items = lexeme_data.root_items_for_line(line_ix);
            for pair in root_items.windows(2) {
                let bigram = Bigram {
                    left: pair[0].1.clone(),
                    right: pair[1].1.clone(),
                };
                let location = (line_ix, pair[0].0);
                bigram_data
                    .bigrams_to_locations
                    .entry(bigram.clone())
                    .or_default()
                    .insert(location);
                bigram_data.add_bigram(&bigram, 1);
            }
        }
        bigram_data
    }

    fn add_bigram(&mut self, bigram: &Bigram, delta: i64) {
        *self.left_lex_freqs.entry(bigram.left.clone()).or_insert(0) += delta;
        *self
            .right_lex_freqs
            .entry(bigram.right.clone())
            .or_insert(0) += delta;

        let was_present = self.bigrams_to_freqs.contains_key(bigram);
        *self.bigrams_to_freqs.entry(bigram.clone()).or_insert(0) += delta;
        if delta > 0 && !was_present {
            self.bigrams_first_seen_order
                .insert(bigram.clone(), self.next_bigram_order);
            self.next_bigram_order += 1;
        }
    }
}

#[derive(Clone)]
struct WinnerInfo {
    bigram: Bigram,
    merged_lexeme: Lexeme,
    bigram_locations: Vec<Location>,
}

impl WinnerInfo {
    fn from_bigram_with_data(bigram: &Bigram, bigram_data: &BigramData) -> Self {
        let mut all_words = Vec::with_capacity(bigram.left.word.len() + bigram.right.word.len());
        all_words.extend_from_slice(&bigram.left.word);
        all_words.extend_from_slice(&bigram.right.word);

        let mut locations = bigram_data
            .bigrams_to_locations
            .get(bigram)
            .cloned()
            .unwrap_or_default()
            .into_iter()
            .collect::<Vec<_>>();
        locations.sort_unstable();

        Self {
            bigram: bigram.clone(),
            merged_lexeme: Lexeme {
                word: all_words.into_iter().collect(),
                ix: 0,
            },
            bigram_locations: locations,
        }
    }

    fn n_lexemes(&self) -> usize {
        self.merged_lexeme.word.len()
    }

    fn cleaned_bigram_locations(&self) -> Vec<Location> {
        let mut clean_locations = Vec::new();
        let mut ix = 0;
        while ix < self.bigram_locations.len() {
            let line = self.bigram_locations[ix].0;
            let mut token_ix = Vec::new();
            while ix < self.bigram_locations.len() && self.bigram_locations[ix].0 == line {
                token_ix.push(self.bigram_locations[ix].1);
                ix += 1;
            }

            let mut exclude_tokens: HashSet<usize> = HashSet::new();
            for token in token_ix.iter().copied() {
                if exclude_tokens.contains(&token) {
                    continue;
                }
                for candidate in token_ix.iter().copied() {
                    if token <= candidate && candidate < token + self.n_lexemes() {
                        exclude_tokens.insert(candidate);
                    }
                }
                clean_locations.push((line, token));
            }
        }

        clean_locations
    }
}

#[derive(Clone)]
struct CandidateScore {
    score: f64,
    frequency: i64,
}

#[derive(Clone)]
struct StepData {
    score: f64,
    winner: WinnerInfo,
    line_hits_count: usize,
}

enum StepStatus {
    Winner(StepData),
    NoCandidate,
    BelowMinScore(f64),
}

fn safe_ll_term(observed: f64, expected: f64) -> f64 {
    if observed > 0.0 {
        observed * (((observed / (expected + SMALL)) + SMALL).ln())
    } else {
        0.0
    }
}

fn scores_close(a: f64, b: f64) -> bool {
    (a - b).abs() <= (SCORE_ATOL + SCORE_RTOL * b.abs())
}

fn is_close_default(a: f64, b: f64) -> bool {
    (a - b).abs() <= 1e-8 + (1e-5 * b.abs())
}

fn coerce_score(score: f64) -> f64 {
    if score.is_finite() {
        score
    } else {
        f64::NEG_INFINITY
    }
}

fn score_ll_npmi(
    method: SelectionMethod,
    bigram_freq: i64,
    left_freq: i64,
    right_freq: i64,
    total_bigram_count: i64,
) -> f64 {
    if total_bigram_count == 0 {
        return f64::NEG_INFINITY;
    }

    let total = total_bigram_count as f64;

    match method {
        SelectionMethod::LogLikelihood => {
            let obs_a = bigram_freq as f64;
            let obs_b = left_freq as f64 - obs_a;
            let obs_c = right_freq as f64 - obs_a;
            let mut obs_d = total - obs_a - obs_b - obs_c;
            if obs_d < 0.0 {
                obs_d = 0.0;
            }

            let exp_a = ((obs_a + obs_b) * (obs_a + obs_c)) / total;
            let exp_b = ((obs_a + obs_b) * (obs_b + obs_d)) / total;
            let exp_c = ((obs_c + obs_d) * (obs_a + obs_c)) / total;
            let exp_d = ((obs_c + obs_d) * (obs_b + obs_d)) / total;

            let ll_a = safe_ll_term(obs_a, exp_a);
            let ll_b = safe_ll_term(obs_b, exp_b);
            let ll_c = safe_ll_term(obs_c, exp_c);
            let ll_d = safe_ll_term(obs_d, exp_d);
            let ll = 2.0 * (ll_a + ll_b + ll_c + ll_d);
            coerce_score(if obs_a > exp_a { ll } else { -ll })
        }
        SelectionMethod::Npmi => {
            let prob_ab = bigram_freq as f64 / total;
            let prob_a = left_freq as f64 / total;
            let prob_b = right_freq as f64 / total;
            let numerator = (prob_ab / (prob_a * prob_b)).ln();
            let denominator = -(prob_ab.ln());
            let npmi = if denominator > 0.0 {
                numerator / denominator
            } else {
                f64::NAN
            };
            let perfect_association =
                is_close_default(denominator, 0.0) && is_close_default(numerator, 0.0);
            coerce_score(if perfect_association { 1.0 } else { npmi })
        }
        SelectionMethod::Frequency => bigram_freq as f64,
    }
}

fn merged_word_ids(bigram: &Bigram) -> SmallVec<[TokenId; 6]> {
    let mut merged =
        SmallVec::<[TokenId; 6]>::with_capacity(bigram.left.word.len() + bigram.right.word.len());
    merged.extend_from_slice(&bigram.left.word);
    merged.extend_from_slice(&bigram.right.word);
    merged
}

fn select_candidate(
    tie_breaker: TieBreaker,
    method: SelectionMethod,
    bigrams_to_freqs: &HashMap<Bigram, i64>,
    bigrams_first_seen_order: &HashMap<Bigram, usize>,
    candidate_scores: &HashMap<Bigram, CandidateScore>,
    min_count: i64,
) -> Option<(Bigram, CandidateScore)> {
    if method == SelectionMethod::Frequency {
        let mut best: Option<(Bigram, CandidateScore)> = None;
        let mut best_key: Option<(Reverse<i64>, SmallVec<[TokenId; 6]>)> = None;
        let mut best_score = f64::NEG_INFINITY;
        let mut best_order = usize::MAX;

        for (bigram, freq) in bigrams_to_freqs {
            if *freq < min_count {
                continue;
            }
            let score = *freq as f64;
            let candidate = CandidateScore {
                score,
                frequency: *freq,
            };
            let order = *bigrams_first_seen_order.get(bigram).unwrap_or(&usize::MAX);

            if best.is_none() || score > best_score {
                best_score = score;
                best_order = order;
                best = Some((bigram.clone(), candidate.clone()));
                if tie_breaker == TieBreaker::Deterministic {
                    best_key = Some((Reverse(*freq), merged_word_ids(bigram)));
                }
                continue;
            }

            if scores_close(score, best_score) {
                if tie_breaker == TieBreaker::LegacyFirstSeen {
                    if order < best_order {
                        best_order = order;
                        best = Some((bigram.clone(), candidate));
                    }
                } else {
                    let candidate_key = (Reverse(*freq), merged_word_ids(bigram));
                    if let Some(key) = &best_key {
                        if candidate_key < *key {
                            best = Some((bigram.clone(), candidate));
                            best_key = Some(candidate_key);
                        }
                    }
                }
            }
        }

        return best;
    }

    if candidate_scores.is_empty() {
        return None;
    }

    let mut best: Option<(Bigram, CandidateScore)> = None;
    let mut best_key: Option<(Reverse<i64>, SmallVec<[TokenId; 6]>)> = None;
    let mut best_score = f64::NEG_INFINITY;
    let mut best_order = usize::MAX;

    for (bigram, candidate) in candidate_scores {
        let order = *bigrams_first_seen_order.get(bigram).unwrap_or(&usize::MAX);

        if best.is_none() || candidate.score > best_score {
            best_score = candidate.score;
            best_order = order;
            best = Some((bigram.clone(), candidate.clone()));
            if tie_breaker == TieBreaker::Deterministic {
                best_key = Some((Reverse(candidate.frequency), merged_word_ids(bigram)));
            }
            continue;
        }

        if scores_close(candidate.score, best_score) {
            if tie_breaker == TieBreaker::LegacyFirstSeen {
                if order < best_order {
                    best_order = order;
                    best = Some((bigram.clone(), candidate.clone()));
                }
            } else {
                let candidate_key = (Reverse(candidate.frequency), merged_word_ids(bigram));
                if let Some(key) = &best_key {
                    if candidate_key < *key {
                        best = Some((bigram.clone(), candidate.clone()));
                        best_key = Some(candidate_key);
                    }
                }
            }
        }
    }

    best
}

fn merge_winner(
    winner: &WinnerInfo,
    clean_locations: &[Location],
    lexeme_data: &mut LexemeData,
    bigram_data: &mut BigramData,
) -> HashSet<Bigram> {
    let mut bigram_lines = HashSet::new();
    for (line_ix, _) in clean_locations {
        bigram_lines.insert(*line_ix);
    }

    let mut touched_lexemes = HashSet::new();
    touched_lexemes.insert(winner.merged_lexeme.clone());
    touched_lexemes.insert(winner.bigram.left.clone());
    touched_lexemes.insert(winner.bigram.right.clone());

    let mut touched_bigrams = HashSet::new();
    touched_bigrams.insert(winner.bigram.clone());

    let mut old_bigrams_lookup: HashMap<usize, Vec<(usize, Lexeme)>> = HashMap::new();
    for line_ix in bigram_lines {
        old_bigrams_lookup.insert(line_ix, lexeme_data.root_items_for_line(line_ix));
    }

    for (line_ix, word_ix) in clean_locations.iter().copied() {
        for lexeme_index in 0..winner.n_lexemes() {
            let pos = word_ix + lexeme_index;
            let old_lexeme = lexeme_data.locations_to_lexemes[line_ix][pos].clone();
            touched_lexemes.insert(old_lexeme.clone());

            let lexeme = Lexeme {
                word: winner.merged_lexeme.word.clone(),
                ix: lexeme_index,
            };
            lexeme_data.locations_to_lexemes[line_ix][pos] = lexeme.clone();

            if let Some(locations) = lexeme_data.lexemes_to_locations.get_mut(&old_lexeme) {
                locations.remove(&(line_ix, pos));
            }
            lexeme_data
                .lexemes_to_locations
                .entry(lexeme)
                .or_default()
                .insert((line_ix, pos));
        }
    }

    for (line_ix, old_root_items) in old_bigrams_lookup {
        let old_bigrams = old_root_items
            .windows(2)
            .map(|pair| {
                (
                    Bigram {
                        left: pair[0].1.clone(),
                        right: pair[1].1.clone(),
                    },
                    (line_ix, pair[0].0),
                )
            })
            .collect::<Vec<_>>();

        let new_root_items = lexeme_data.root_items_for_line(line_ix);
        let new_bigrams = new_root_items
            .windows(2)
            .map(|pair| {
                (
                    Bigram {
                        left: pair[0].1.clone(),
                        right: pair[1].1.clone(),
                    },
                    (line_ix, pair[0].0),
                )
            })
            .collect::<Vec<_>>();

        for (bigram, _) in &old_bigrams {
            touched_bigrams.insert(bigram.clone());
        }
        for (bigram, _) in &new_bigrams {
            touched_bigrams.insert(bigram.clone());
        }

        for (bigram, _) in &new_bigrams {
            bigram_data.add_bigram(bigram, 1);
        }
        for (bigram, _) in &old_bigrams {
            bigram_data.add_bigram(bigram, -1);
        }

        for (bigram, location) in old_bigrams {
            if let Some(locations) = bigram_data.bigrams_to_locations.get_mut(&bigram) {
                locations.remove(&location);
            }
        }
        for (bigram, location) in new_bigrams {
            bigram_data
                .bigrams_to_locations
                .entry(bigram)
                .or_default()
                .insert(location);
        }
    }

    let merge_token_count = clean_locations.len() as i64;
    lexeme_data
        .lexemes_to_freqs
        .insert(winner.merged_lexeme.clone(), merge_token_count);

    if let Some(el1_freq) = lexeme_data.lexemes_to_freqs.get_mut(&winner.bigram.left) {
        *el1_freq -= merge_token_count;
    }
    if let Some(el2_freq) = lexeme_data.lexemes_to_freqs.get_mut(&winner.bigram.right) {
        *el2_freq -= merge_token_count;
    }

    for lexeme in touched_lexemes {
        let remove_freq = lexeme_data
            .lexemes_to_freqs
            .get(&lexeme)
            .map(|freq| *freq <= 0)
            .unwrap_or(false);
        if remove_freq {
            lexeme_data.lexemes_to_freqs.remove(&lexeme);
        }

        let remove_locations = lexeme_data
            .lexemes_to_locations
            .get(&lexeme)
            .map(|locations| locations.is_empty())
            .unwrap_or(true);
        if remove_locations {
            lexeme_data.lexemes_to_locations.remove(&lexeme);
        }
    }

    let mut touched_lr_lexemes = HashSet::new();
    for bigram in &touched_bigrams {
        touched_lr_lexemes.insert(bigram.left.clone());
        touched_lr_lexemes.insert(bigram.right.clone());

        let remove_freq = bigram_data
            .bigrams_to_freqs
            .get(bigram)
            .map(|freq| *freq <= 0)
            .unwrap_or(false);
        if remove_freq {
            bigram_data.bigrams_to_freqs.remove(bigram);
            bigram_data.bigrams_first_seen_order.remove(bigram);
        }

        let remove_locations = bigram_data
            .bigrams_to_locations
            .get(bigram)
            .map(|locations| locations.is_empty())
            .unwrap_or(true);
        if remove_locations {
            bigram_data.bigrams_to_locations.remove(bigram);
        }
    }

    for lexeme in touched_lr_lexemes {
        let remove_left = bigram_data
            .left_lex_freqs
            .get(&lexeme)
            .map(|freq| *freq <= 0)
            .unwrap_or(false);
        if remove_left {
            bigram_data.left_lex_freqs.remove(&lexeme);
        }

        let remove_right = bigram_data
            .right_lex_freqs
            .get(&lexeme)
            .map(|freq| *freq <= 0)
            .unwrap_or(false);
        if remove_right {
            bigram_data.right_lex_freqs.remove(&lexeme);
        }
    }

    touched_bigrams
}

#[pyclass]
struct Engine {
    interner: Interner,
    lexemes: LexemeData,
    bigrams: BigramData,
    method: SelectionMethod,
    min_count: i64,
    tie_breaker: TieBreaker,
    candidate_scores: HashMap<Bigram, CandidateScore>,
    dirty_bigrams: HashSet<Bigram>,
    iteration_counter: usize,
}

impl Engine {
    fn refresh_candidate_scores(&mut self, force_full: bool) {
        if self.method == SelectionMethod::Frequency {
            return;
        }

        #[allow(clippy::manual_is_multiple_of)]
        let full = force_full
            || self.candidate_scores.is_empty()
            || self.iteration_counter % FULL_RESCORE_INTERVAL == 0;

        let total_bigram_count = self.bigrams.bigrams_to_freqs.values().sum::<i64>();

        if full {
            let snapshot = self
                .bigrams
                .bigrams_to_freqs
                .iter()
                .filter(|(_, freq)| **freq >= self.min_count)
                .map(|(bigram, freq)| {
                    (
                        bigram.clone(),
                        *freq,
                        *self.bigrams.left_lex_freqs.get(&bigram.left).unwrap_or(&0),
                        *self
                            .bigrams
                            .right_lex_freqs
                            .get(&bigram.right)
                            .unwrap_or(&0),
                    )
                })
                .collect::<Vec<_>>();

            let scored = snapshot
                .par_iter()
                .map(|(bigram, freq, left_freq, right_freq)| {
                    (
                        bigram.clone(),
                        CandidateScore {
                            score: score_ll_npmi(
                                self.method,
                                *freq,
                                *left_freq,
                                *right_freq,
                                total_bigram_count,
                            ),
                            frequency: *freq,
                        },
                    )
                })
                .collect::<Vec<_>>();

            self.candidate_scores.clear();
            for (bigram, score) in scored {
                if score.score > f64::NEG_INFINITY {
                    self.candidate_scores.insert(bigram, score);
                }
            }
            self.dirty_bigrams.clear();
            return;
        }

        let dirty = self.dirty_bigrams.drain().collect::<Vec<_>>();
        let dirty_snapshot = dirty
            .into_iter()
            .map(|bigram| {
                (
                    bigram.clone(),
                    self.bigrams.bigrams_to_freqs.get(&bigram).copied(),
                    self.bigrams.left_lex_freqs.get(&bigram.left).copied(),
                    self.bigrams.right_lex_freqs.get(&bigram.right).copied(),
                )
            })
            .collect::<Vec<_>>();

        let rescored = dirty_snapshot
            .par_iter()
            .map(|(bigram, maybe_freq, maybe_left, maybe_right)| {
                let Some(freq) = maybe_freq else {
                    return (bigram.clone(), None);
                };
                if *freq < self.min_count {
                    return (bigram.clone(), None);
                }
                let left_freq = maybe_left.unwrap_or(0);
                let right_freq = maybe_right.unwrap_or(0);
                let score = score_ll_npmi(
                    self.method,
                    *freq,
                    left_freq,
                    right_freq,
                    total_bigram_count,
                );
                if score == f64::NEG_INFINITY {
                    (bigram.clone(), None)
                } else {
                    (
                        bigram.clone(),
                        Some(CandidateScore {
                            score,
                            frequency: *freq,
                        }),
                    )
                }
            })
            .collect::<Vec<_>>();

        for (bigram, maybe_score) in rescored {
            if let Some(score) = maybe_score {
                self.candidate_scores.insert(bigram, score);
            } else {
                self.candidate_scores.remove(&bigram);
            }
        }
    }

    fn token_ids_to_strings(&self, token_ids: &[TokenId]) -> Vec<String> {
        self.interner.ids_to_strings(token_ids)
    }

    fn step_payload(&self, step_data: StepData) -> StepPayload {
        (
            step_data.score,
            self.token_ids_to_strings(&step_data.winner.bigram.left.word),
            step_data.winner.bigram.left.ix,
            self.token_ids_to_strings(&step_data.winner.bigram.right.word),
            step_data.winner.bigram.right.ix,
            self.token_ids_to_strings(&step_data.winner.merged_lexeme.word),
            step_data.winner.merged_lexeme.ix,
            step_data.winner.bigram_locations,
        )
    }

    fn step_internal(&mut self, min_score: Option<f64>) -> StepStatus {
        self.refresh_candidate_scores(false);

        let Some((bigram, candidate)) = select_candidate(
            self.tie_breaker,
            self.method,
            &self.bigrams.bigrams_to_freqs,
            &self.bigrams.bigrams_first_seen_order,
            &self.candidate_scores,
            self.min_count,
        ) else {
            return StepStatus::NoCandidate;
        };

        if let Some(score_threshold) = min_score {
            if candidate.score < score_threshold {
                return StepStatus::BelowMinScore(candidate.score);
            }
        }

        let winner = WinnerInfo::from_bigram_with_data(&bigram, &self.bigrams);
        let clean_locations = winner.cleaned_bigram_locations();
        let line_hits_count = clean_locations
            .iter()
            .map(|(line_ix, _)| *line_ix)
            .collect::<HashSet<_>>()
            .len();

        let touched = merge_winner(
            &winner,
            &clean_locations,
            &mut self.lexemes,
            &mut self.bigrams,
        );
        self.dirty_bigrams = touched;
        self.iteration_counter += 1;

        StepStatus::Winner(StepData {
            score: candidate.score,
            winner,
            line_hits_count,
        })
    }

    fn run_internal(
        &mut self,
        iterations: usize,
        min_score: Option<f64>,
        return_progress: bool,
    ) -> RunOutcome {
        let mut winners = Vec::new();
        let mut progress = Vec::new();
        let corpus_length = self.lexemes.corpus_length();

        for _ in 0..iterations {
            match self.step_internal(min_score) {
                StepStatus::NoCandidate => {
                    return (
                        "no_candidate".to_string(),
                        winners,
                        None,
                        corpus_length,
                        progress,
                    )
                }
                StepStatus::BelowMinScore(score) => {
                    return (
                        "below_min_score".to_string(),
                        winners,
                        Some(score),
                        corpus_length,
                        progress,
                    )
                }
                StepStatus::Winner(step_data) => {
                    if return_progress {
                        progress.push((
                            step_data.line_hits_count,
                            step_data.score,
                            self.token_ids_to_strings(&step_data.winner.merged_lexeme.word),
                        ));
                    }
                    winners.push(self.step_payload(step_data));
                }
            }
        }

        (
            "completed".to_string(),
            winners,
            None,
            corpus_length,
            progress,
        )
    }
}

type StepPayload = (
    f64,
    Vec<String>,
    usize,
    Vec<String>,
    usize,
    Vec<String>,
    usize,
    Vec<(usize, usize)>,
);

type ProgressPayload = (usize, f64, Vec<String>);
type RunOutcome = (
    String,
    Vec<StepPayload>,
    Option<f64>,
    usize,
    Vec<ProgressPayload>,
);

#[pymethods]
impl Engine {
    #[new]
    fn new(
        corpus: Vec<Vec<String>>,
        method: &str,
        min_count: usize,
        tie_breaker: &str,
    ) -> PyResult<Self> {
        let method = SelectionMethod::parse(method)?;
        let tie_breaker = TieBreaker::parse(tie_breaker)?;
        let interner = Interner::from_corpus(&corpus);
        let corpus_ids = corpus
            .iter()
            .map(|line| {
                line.iter()
                    .map(|token| interner.id_for(token))
                    .collect::<Vec<_>>()
            })
            .collect::<Vec<_>>();

        let lexemes = LexemeData::from_corpus(&corpus_ids);
        let bigrams = BigramData::from_lexemes(&lexemes);

        Ok(Self {
            interner,
            lexemes,
            bigrams,
            method,
            min_count: min_count as i64,
            tie_breaker,
            candidate_scores: HashMap::new(),
            dirty_bigrams: HashSet::new(),
            iteration_counter: 0,
        })
    }

    fn corpus_length(&self) -> usize {
        self.lexemes.corpus_length()
    }

    #[pyo3(signature = (iterations, min_score=None, return_progress=false))]
    fn run(
        &mut self,
        py: Python<'_>,
        iterations: usize,
        min_score: Option<f64>,
        return_progress: bool,
    ) -> RunOutcome {
        py.allow_threads(|| self.run_internal(iterations, min_score, return_progress))
    }
}

#[pymodule(gil_used = true)]
fn _core(_py: Python<'_>, module: &Bound<'_, PyModule>) -> PyResult<()> {
    module.add_class::<Engine>()?;
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    fn build_engine(corpus: Vec<Vec<&str>>, method: SelectionMethod, min_count: i64) -> Engine {
        let corpus = corpus
            .into_iter()
            .map(|line| line.into_iter().map(str::to_string).collect::<Vec<_>>())
            .collect::<Vec<_>>();

        let interner = Interner::from_corpus(&corpus);
        let corpus_ids = corpus
            .iter()
            .map(|line| {
                line.iter()
                    .map(|token| interner.id_for(token))
                    .collect::<Vec<_>>()
            })
            .collect::<Vec<_>>();
        let lexemes = LexemeData::from_corpus(&corpus_ids);
        let bigrams = BigramData::from_lexemes(&lexemes);

        Engine {
            interner,
            lexemes,
            bigrams,
            method,
            min_count,
            tie_breaker: TieBreaker::Deterministic,
            candidate_scores: HashMap::new(),
            dirty_bigrams: HashSet::new(),
            iteration_counter: 0,
        }
    }

    #[test]
    fn interner_roundtrip() {
        let corpus = vec![vec!["b".to_string(), "a".to_string(), "b".to_string()]];
        let interner = Interner::from_corpus(&corpus);
        let ids = vec![interner.id_for("a"), interner.id_for("b")];
        assert_eq!(interner.ids_to_strings(&ids), vec!["a", "b"]);
        assert!(interner.id_for("a") < interner.id_for("b"));
    }

    #[test]
    fn deterministic_tie_break_prefers_frequency_then_lexicographic() {
        let mut engine = build_engine(
            vec![
                vec!["a", "d"],
                vec!["a", "c"],
                vec!["a", "b"],
                vec!["a", "b"],
            ],
            SelectionMethod::Frequency,
            0,
        );
        let StepStatus::Winner(step) = engine.step_internal(None) else {
            panic!("expected winner");
        };
        let merged = engine.token_ids_to_strings(&step.winner.merged_lexeme.word);
        assert_eq!(merged, vec!["a", "b"]);
    }

    #[test]
    fn engine_run_matches_repeated_step_for_frequency() {
        let corpus = vec![vec!["a", "a", "a", "a"]];
        let mut run_engine = build_engine(corpus.clone(), SelectionMethod::Frequency, 0);
        let mut step_engine = build_engine(corpus, SelectionMethod::Frequency, 0);

        let (_, run_payloads, _, _, _) = run_engine.run_internal(3, None, false);

        let mut step_payloads = Vec::new();
        for _ in 0..3 {
            match step_engine.step_internal(None) {
                StepStatus::Winner(step_data) => {
                    step_payloads.push(step_engine.step_payload(step_data))
                }
                _ => break,
            }
        }

        assert_eq!(run_payloads, step_payloads);
    }

    #[test]
    fn min_score_blocks_low_score_winner() {
        let mut engine = build_engine(vec![vec!["a", "b", "c"]], SelectionMethod::Frequency, 0);
        let status = engine.step_internal(Some(10.0));
        let StepStatus::BelowMinScore(score) = status else {
            panic!("expected below-min-score status");
        };
        assert_eq!(score, 1.0);
    }
}
</file>

<file path="tests/fixtures.py">
from pathlib import Path

import pytest


@pytest.fixture(scope="session")
def sample_corpus():
    """We'll use the corpus included in the original implementation,
    which is "[...] a combination of the Santa Barbara
    Corpus of Spoken American English and the spoken
    component of the ICE Canada corpus"

    > Du Bois, John W., Wallace L. Chafe, Charles Meyers, Sandra A. Thompson, Nii Martey, and Robert Englebretson (2005). Santa Barbara corpus of spoken American English. Philadelphia: Linguistic Data Consortium.

    > Newman, John and Georgie Columbus (2010). The International Corpus of English – Canada. Edmonton, Alberta: University of Alberta.

    Note that these are dialogue corpuses, so each line is often referred to as a `turn`. They've also been pre-processed with lowercasing, and punctuation replaced with alphanumeric substitutions (`_` --> `undrscr`).
    """
    corpus: list[list[str]] = []
    this_folder = Path(__file__).parent
    txt_files = sorted((this_folder / Path("sample_corpus/")).glob("*.TXT"))
    for txt_file in txt_files:
        for line in txt_file.read_text().split("\n"):
            if line:
                tokens: list[str] = line.split(" ")
                corpus.append(tokens)
    return corpus
</file>

<file path="AGENTS.md">
# AGENTS.md

## Project Tooling
- Package and environment management: `uv`
- Linting/formatting: `ruff`
- Type checking: `ty`

## Standard Workflow
1. Sync the environment after pulling or changing dependencies:
   - `uv sync --all-groups`
2. If dependency constraints changed in `pyproject.toml`, refresh lock + sync:
   - `uv lock`
   - `uv sync --all-groups --frozen`
3. After code changes, run quality checks (formatting first):
   - `uv run ruff format src tests`
   - `uv run ruff check src tests`
   - `uv run ty check src tests`
4. Run tests before finishing:
   - `uv run --no-sync pytest -v -m "not corpus and not parity"`

## Rust/PyO3 Fast Loop
1. Recommended command order for quickest feedback:
   - `cargo check`
   - `cargo test`
   - `uv run --no-sync maturin develop`
   - `uv run --no-sync pytest -q tests/test_smoke.py`
   - `uv run --no-sync pytest -q -m "fast"`
2. Build/install the Rust extension after Rust code changes:
   - `uv run --no-sync maturin develop`
3. Run targeted tests while iterating:
   - `uv run --no-sync pytest -q -m "fast" tests/test_remerge.py -k "<pattern>"`
4. If behavior looks stale, rebuild the extension first:
   - `uv run --no-sync maturin develop`
5. If PyO3 build detection seems wrong, print config and verify interpreter:
   - `PYO3_PRINT_CONFIG=1 uv run --no-sync maturin develop`
   - `PYO3_PYTHON=.venv/bin/python uv run --no-sync maturin develop`
6. If `cargo test` fails with unresolved Python symbols, verify `Cargo.toml` is not forcing `pyo3/extension-module` and rerun:
   - `cargo clean`
   - `cargo test`

## Full Verification Before Handoff
1. Python checks:
   - `uv run ruff format src tests`
   - `uv run ruff check src tests`
   - `uv run ty check src tests`
   - `uv run --no-sync pytest -v -m "not corpus and not parity"`
   - `uv run --no-sync pytest -v -m "corpus or parity"`
2. Rust checks:
   - `cargo fmt --all`
   - `cargo clippy --all-targets -- -D warnings`
   - `cargo test`
</file>

<file path=".github/workflows/tests.yml">
on:
  push:
    branches: [main]
  pull_request:

jobs:
  fast-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - uses: dtolnay/rust-toolchain@stable
      - uses: astral-sh/setup-uv@v4
      - name: cache uv
        uses: actions/cache@v4
        with:
          path: ~/.cache/uv
          key: uv-${{ runner.os }}-py312-${{ hashFiles('**/uv.lock') }}
      - run: uv sync --all-groups --frozen
      - run: cargo check
      - run: cargo test
      - run: cargo clippy --all-targets -- -D warnings
      - run: uv run --with maturin maturin develop
      - run: uv run --no-sync pytest -q tests/test_smoke.py
      - run: uv run --with maturin maturin build --release -o dist
      - run: |
          python -m venv .wheel-smoke
          . .wheel-smoke/bin/activate
          pip install --upgrade pip
          pip install dist/remerge_mwe-*.whl
          python -c "import remerge; assert callable(remerge.run); assert remerge.run([['a','b']], 1, method='frequency', progress_bar='none')[0].merged_lexeme.word == ('a', 'b')"
      - run: uv run --no-sync pytest -v -m "not corpus and not parity"

  full-corpus:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: fast-test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - uses: dtolnay/rust-toolchain@stable
      - uses: astral-sh/setup-uv@v4
      - name: cache uv
        uses: actions/cache@v4
        with:
          path: ~/.cache/uv
          key: uv-${{ runner.os }}-py312-${{ hashFiles('**/uv.lock') }}
      - run: uv sync --all-groups --frozen
      - run: uv run --with maturin maturin develop
      - run: uv run --no-sync pytest -v -m "corpus or parity"
</file>

<file path="src/remerge/__init__.py">
from .core import (
    ExhaustionPolicy,
    NoCandidateBigramError,
    SelectionMethod,
    TieBreaker,
    run,
)

__version__ = "0.2.1"

__all__ = [
    "run",
    "SelectionMethod",
    "TieBreaker",
    "ExhaustionPolicy",
    "NoCandidateBigramError",
]
</file>

<file path="pyproject.toml">
[project]
name = "remerge-mwe"
version = "0.2.1"
description = "REMERGE is a Multi-Word Expression (MWE) discovery algorithm derived from the MERGE algorithm."
authors = [{name = "Peter Baumgartner", email = "5107405+pmbaumgartner@users.noreply.github.com"}]
readme = "README.md"
license = "MIT"
requires-python = ">=3.12"
dependencies = [
    "tqdm>=4.64.1,<5",
    "numpy>=2,<3",
]

[dependency-groups]
dev = [
    "pytest>=7.1.3,<8",
    "ipython>=8.5.0,<9",
    "ruff>=0.9,<1",
    "ty>=0.0.1a16",
    "py-spy>=0.3.14,<0.4",
    "pytest-sugar>=0.9.5,<0.10",
    "pytest-cov>=4.0.0,<5",
]

[tool.maturin]
python-source = "src"
module-name = "remerge._core"
features = ["python-extension"]
profile = "release"
editable-profile = "dev"

[build-system]
requires = ["maturin>=1.8,<2.0"]
build-backend = "maturin"

[tool.pytest.ini_options]
markers = [
    "fast: quick, small-corpus tests for local iteration",
    "corpus: tests that exercise the bundled sample corpus",
    "parity: full-corpus regression parity checks (slow)",
]
</file>

<file path="tests/test_remerge.py">
import json
from pathlib import Path

import pytest
from remerge import __version__, run
from remerge.core import Lexeme, NoCandidateBigramError, SelectionMethod


def _summarize_winners(winners):
    return [
        {
            "merged_word": list(winner.merged_lexeme.word),
            "merge_token_count": winner.merge_token_count,
        }
        for winner in winners
    ]


def test_version():
    assert __version__ == "0.2.1"


@pytest.mark.corpus
def test_sample_corpus_single_iter(sample_corpus):
    winners = run(sample_corpus, 1, progress_bar="none")
    assert winners[0].merged_lexeme == Lexeme(("you", "know"), 0)


@pytest.mark.fast
def test_winner_shape():
    winners = run([["a", "b", "c"]], 1, method="frequency", progress_bar="none")
    winner = winners[0]
    assert winner.bigram[0] == Lexeme(("a",), 0)
    assert winner.bigram[1] == Lexeme(("b",), 0)
    assert winner.merged_lexeme == Lexeme(("a", "b"), 0)
    assert winner.n_lexemes == 2
    assert winner.merge_token_count == 1


@pytest.mark.fast
def test_consecutive_single():
    """Ensure greedy bigram merge avoids overlapping middle bigram."""
    corpus = [["a", "a", "a", "a"]]
    winners = run(corpus, 2, progress_bar="none")
    assert winners[0].merge_token_count == 2
    assert winners[1].merged_lexeme == Lexeme(("a", "a", "a", "a"), 0)


@pytest.mark.fast
def test_consecutive_remainder():
    """Ensure greedy bigram merge avoids overlapping trailing bigram."""
    corpus = [["c", "a", "b", "a", "b", "a", "b", "d"]]
    winners = run(corpus, 2, method="frequency", progress_bar="none")
    assert winners[0].merge_token_count == 3
    assert winners[1].merge_token_count == 1


@pytest.mark.fast
@pytest.mark.parametrize("progress_bar", ["all", "iterations", "none"])
def test_progress_bar(progress_bar):
    corpus = [["a", "b", "a", "b", "c"], ["a", "b", "d", "e"]]
    winners = run(corpus, 2, progress_bar=progress_bar)
    assert len(winners) == 2


@pytest.mark.fast
@pytest.mark.parametrize(
    "method,min_count",
    [
        (SelectionMethod.log_likelihood, 0),
        (SelectionMethod.npmi, 0),
        (SelectionMethod.frequency, 0),
        ("frequency", 0),
    ],
)
def test_methods(method, min_count):
    corpus = [["a", "b", "a", "b", "c"], ["a", "b", "d", "e"]]
    winners = run(corpus, 2, method=method, min_count=min_count, progress_bar="none")
    assert len(winners) == 2


@pytest.mark.fast
def test_output(tmp_path):
    output = tmp_path / "tmp.json"
    winners = run([["a", "b", "a", "b"]], 1, output=output, progress_bar="none")
    results = json.loads(output.read_text())
    assert tuple(results["0"]) == winners[0].merged_lexeme.word


@pytest.mark.fast
def test_output_writes_once_by_default(monkeypatch, tmp_path):
    output = tmp_path / "tmp.json"
    corpus = [["a", "a", "a", "a"]]
    writes = []
    original_write_text = Path.write_text

    def _capture_write(path_obj, text, *args, **kwargs):
        if path_obj == output:
            writes.append(text)
        return original_write_text(path_obj, text, *args, **kwargs)

    monkeypatch.setattr(Path, "write_text", _capture_write)
    winners = run(corpus, 2, output=output, progress_bar="none")

    assert len(writes) == 1
    expected = {str(j): list(w.merged_lexeme.word) for j, w in enumerate(winners)}
    assert json.loads(writes[0]) == expected
    assert json.loads(output.read_text()) == expected


@pytest.mark.fast
def test_output_debug_writes_each_iteration_with_cumulative_content(
    monkeypatch, tmp_path
):
    output = tmp_path / "tmp.json"
    corpus = [["a", "a", "a", "a"]]
    writes = []
    original_write_text = Path.write_text

    def _capture_write(path_obj, text, *args, **kwargs):
        if path_obj == output:
            writes.append(text)
        return original_write_text(path_obj, text, *args, **kwargs)

    monkeypatch.setattr(Path, "write_text", _capture_write)
    winners = run(
        corpus,
        2,
        output=output,
        output_debug_each_iteration=True,
        progress_bar="none",
    )

    assert len(writes) == len(winners)
    for i, payload in enumerate(writes):
        decoded = json.loads(payload)
        expected = {
            str(j): list(w.merged_lexeme.word) for j, w in enumerate(winners[: i + 1])
        }
        assert decoded == expected


@pytest.mark.fast
def test_empty_or_single_token_corpus_stops_cleanly():
    assert run([], 1, progress_bar="none") == []
    assert run([["a"]], 5, progress_bar="none") == []


@pytest.mark.fast
def test_exhausted_policy_raise():
    with pytest.raises(NoCandidateBigramError):
        run([["a"]], 1, on_exhausted="raise", progress_bar="none")


@pytest.mark.fast
def test_iterations_larger_than_available_merges_stop():
    winners = run([["a", "b", "c"]], 99, progress_bar="none")
    assert len(winners) == 2


@pytest.mark.fast
def test_frequency_respects_min_count():
    corpus = [["a", "b"], ["a", "c"]]
    winners = run(
        corpus,
        1,
        method="frequency",
        min_count=2,
        on_exhausted="stop",
        progress_bar="none",
    )
    assert winners == []


@pytest.mark.fast
@pytest.mark.parametrize("method", ["frequency", "log_likelihood", "npmi"])
def test_deterministic_tie_breaking_is_order_independent(method):
    corpus_a = [["a", "b"], ["c", "d"]]
    corpus_b = [["c", "d"], ["a", "b"]]

    winner_a = run(corpus_a, 1, method=method, progress_bar="none")[
        0
    ].merged_lexeme.word
    winner_b = run(corpus_b, 1, method=method, progress_bar="none")[
        0
    ].merged_lexeme.word
    assert winner_a == winner_b == ("a", "b")


@pytest.mark.fast
def test_legacy_tie_breaking_keeps_first_seen_behavior():
    corpus_a = [["a", "b"], ["c", "d"]]
    corpus_b = [["c", "d"], ["a", "b"]]

    winner_a = run(
        corpus_a,
        1,
        method="frequency",
        tie_breaker="legacy_first_seen",
        progress_bar="none",
    )[0].merged_lexeme.word
    winner_b = run(
        corpus_b,
        1,
        method="frequency",
        tie_breaker="legacy_first_seen",
        progress_bar="none",
    )[0].merged_lexeme.word

    assert winner_a != winner_b


@pytest.mark.fast
def test_min_score_stops_or_raises():
    corpus = [["a", "b", "c"]]
    assert run(corpus, 2, min_score=1e9, on_exhausted="stop", progress_bar="none") == []

    with pytest.raises(NoCandidateBigramError):
        run(corpus, 2, min_score=1e9, on_exhausted="raise", progress_bar="none")


@pytest.mark.parity
def test_parity_fixture(sample_corpus):
    parity = json.loads(Path("tests/parity_expected.json").read_text())

    ll = run(
        sample_corpus, 5, method="log_likelihood", min_count=0, progress_bar="none"
    )
    freq = run(sample_corpus, 5, method="frequency", min_count=0, progress_bar="none")
    npmi = run(sample_corpus, 5, method="npmi", min_count=25, progress_bar="none")

    assert _summarize_winners(ll) == parity["log_likelihood"]
    assert _summarize_winners(freq) == parity["frequency"]
    assert _summarize_winners(npmi) == parity["npmi"]

    tie_corpus = [["a", "b"], ["c", "d"]]
    for method, expected in parity["tie_deterministic"].items():
        winner = run(tie_corpus, 1, method=method, progress_bar="none")[0]
        assert list(winner.merged_lexeme.word) == expected
</file>

<file path="README.md">
# REMERGE - Multi-Word Expression discovery algorithm

REMERGE is a Multi-Word Expression (MWE) discovery algorithm, which started as a re-implementation and simplification of a similar algorithm called MERGE, detailed in a publication and PhD thesis[^2][^3]. The primary benefit of this algorithm is that it's non-parametric in regards to the size of the n-grams that constitute a MWE—you do not need to specify a priori how many n-grams comprise a MWE—you only need to specify the number of iterations you want the algorithm to run.

The code was originally derived from an existing implementation from the original author[^1] that I reviewed, converted from python 2 to 3, then modified and updated with the following:
- a correction of the log-likelihood calculation; previously it was not using the correct values for the contingency table
- the removal of gapsize / discontinuous bigrams (see below for issues with the prior implementation)
- an overall reduction in codebase size and complexity
  - ~60% reduction in loc
  - removed `pandas` and `nltk` dependencies
- type annotations
- the inclusion of additional metrics (Frequency, [NPMI](https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf)[^4]) for selecting the winning bigram.
- corrections for merging sequential bigrams greedily and completely.
  - e.g. `'ya ya ya ya'` -> `'(ya ya) (ya ya)'` -> `'(ya ya ya ya)'`. Previously the merge order was non-deterministic, and you could end up with `'ya (ya ya) ya'`
- An overall simplification of the algorithm. 
  - As a tradeoff, this version may be less efficient. After a bigram is merged into a single lexeme in the original implementation, new bigrams and conflicting (old) bigrams were respectively added and subtracted from a mutable counter of bigrams. The counts of this object were difficult to track and validate, and resulted in errors in certain cases, so I removed this step. Instead, only the lexeme data is updated with the new merged lexemes. Then, we track which lines contain the merged lexeme and create an *update* counter that subtracts the old bigrams from the new bigrams and updates the bigram data using that counter.
- [Clarified license with the original author](https://github.com/awahl1/MERGE/commit/0a118df852a573fa6db4cc9aea00fd8c691b52fa) and licensed as MIT.

#### Usage

```python
import remerge

corpus = [
    ["a", "list", "of", "already", "tokenized", "texts"],
    ["where", "each", "item", "is", "a", "list", "of", "tokens"],
    ["isn't", "a", "list", "nice"]
]

winners = remerge.run(
    corpus, iterations=1, method=remerge.SelectionMethod.frequency, progress_bar="all"
)
# winners[0].merged_lexeme.word == ('a', 'list')
```

There are 3 bigram winner selection methods: [Log-Likelihood (G²)](https://aclanthology.org/J93-1003.pdf)[^5], [NPMI](https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf)[^4], and raw frequency. They are available under the `SelectionMethod` enum. The default is log-likelihood, which was used in the original implementation.

If using `NPMI` (`SelectionMethod.npmi`), you likely want to provide a `min_count` parameter, "as infrequent word pairs tend to dominate the top of bigramme lists that are ranked after PMI". (p. 4[^4])

```python
winners = remerge.run(corpus, 100, method=remerge.SelectionMethod.npmi, min_count=25)
```

#### API - `remerge.run`

| Argument     | Type                           | Description                                                                                                                                                                                                                                                                                     |
| ------------ | ------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| corpus       | `List[List[str]]`              | A corpus of already tokenized texts.                                                                                                                                                                                                                                                            |
| iterations   | `int`                          | The maximum number of iterations to run the algorithm. Papers typically use >500.                                                                                                                                                                                                               |
| method       | `SelectionMethod`, optional    | One of "frequency", "log_likelihood", or "npmi". Defaults to "log_likelihood".                                                                                                                                                                                                                  |
| min_count    | `int`, optional                | The minimum count required for a bigram to be included in winner calculations for all methods. If choosing NPMI ("npmi"), prefer using min_count because this measure is biased towards infrequent word pairs. Defaults to 0.                                                               |
| output       | `Optional[Path]`, optional     | A file path to output the winning merged lexemes as JSON. Defaults to None.                                                                                                                                                                                                                     |
| progress_bar | `ProgressBarOptions`, optional | Verbosity of progress bar. "all" will display the lexeme and bigram construction progress each iteration plus total iteration progress. "iterations" will display progress on total iterations. "none" has no output. Defaults to "iterations".                                             |
| tie_breaker  | `TieBreaker`, optional         | How ties are resolved among equal-scoring candidates. "deterministic" ranks by score, then frequency, then lexicographic merged token order. "legacy_first_seen" uses prior first-seen behavior. Defaults to "deterministic".                                                             |
| on_exhausted | `ExhaustionPolicy`, optional   | Behavior when no candidate passes filters (or threshold): "stop" returns winners collected so far, "raise" raises `NoCandidateBigramError`. Defaults to "stop".                                                                                                                              |
| min_score    | `Optional[float]`, optional    | Optional minimum score threshold for the selected winner. If the best candidate is below this threshold, behavior follows `on_exhausted`. Defaults to None.                                                                                                                                   |

#### Install

Latest release:

```bash
pip install -U remerge-mwe
```

For latest from github:

```bash
pip install git+https://github.com/pmbaumgartner/remerge-mwe.git 
```

#### Development

Use [`uv`](https://github.com/astral-sh/uv) for local project and dependency management.
This package now builds a Rust extension via PyO3 + maturin, so a local Rust toolchain is required.

Create/sync the environment with all dependency groups:

```bash
uv sync --all-groups
```

Build/install the extension for the current environment:

```bash
uv run --no-sync maturin develop
```

Run tests:

```bash
uv run --no-sync pytest -v -m "not corpus and not parity"
```

Run full corpus/parity checks (slower, intended for CI/mainline validation):

```bash
uv run --no-sync pytest -v -m "corpus or parity"
```

Add a runtime dependency:

```bash
uv add <pkg>
```

Add a development dependency:

```bash
uv add --dev <pkg>
```

If you make changes under `rust/`, run `uv run --no-sync maturin develop` again before testing.

PyO3 troubleshooting:

```bash
# Print PyO3 interpreter/build config and stop.
PYO3_PRINT_CONFIG=1 uv run --no-sync maturin develop

# Force the Python interpreter PyO3 should inspect.
PYO3_PYTHON=.venv/bin/python uv run --no-sync maturin develop
```

If `cargo test` fails with unresolved Python symbols, confirm `pyo3/extension-module`
is not forced in `Cargo.toml`, then run:

```bash
cargo clean
cargo test
```

#### Rust/PyO3 Backlog

Planned follow-up after stabilization: split into a pure Rust core crate plus a thin
PyO3 bindings crate.

Trigger for this split:
1. Parity tests remain stable across 2 consecutive PRs.
2. CI remains green across 2 consecutive PRs.
3. Benchmark baseline from `bin/benchmark-remerge.sh --build release --runs 1 --iterations 5` is stable across 2 consecutive PRs.

#### How it works

The algorithm operates iteratively in two stages: first, it collects all bigrams of co-occurring `lexemes` in the corpus. A measure is calculated on the set of all bigrams to determine a winner. The two lexemes that comprise the winning bigram are merged into a single lexeme. Instances of that bigram (`lexeme` pair) in the corpus are replaced with the merged lexeme. Outdated bigrams, i.e. those that don't exist anymore because one of their elements is now a merged lexeme, are subtracted from the bigram data. New bigrams, i.e. those where one element is now a merged lexeme, are added to the bigram data. With this new set of bigram data, the process repeats and a new winner is selected.

At initialization, a `lexeme` consists of only a single token, but as the algorithm iterates lexemes become multi-word expressions formed from the winning bigrams. `Lexemes` contain two parts: a `word` which is a tuple of strings, and an `index` which represents the position of that specific token in a MWE. For example, if the winning bigram is `(you, know)`, occurrences of that sequence of lexemes will be replaced with `[(you, know), 0]` and `[(you, know), 1]` in the corpus. When bigrams are counted, only a root lexeme (where the index is 0) can form a bigram, so merged tokens don't get double counted. For a more visual explanation of a few iterations assuming specific winners, see the image below.

<img src="explanation.png" alt="An explanation of the remerge algorithm" width="820">

#### Limitations

This implementation is still a greedy agglomerative procedure, so local winner choices can influence later merges. Different selection methods (`frequency`, `log_likelihood`, `npmi`) can lead to materially different MWE inventories depending on corpus size and domain.

#### Issues with Original Algorithm

##### Single Bigrams with discontinuities forming from distinct Lexeme positions

One issue with discontinuities or gaps in the original algorithm is that it did not distinguish the position of a satellite lexeme occurring to the left or right of a bigram with a gap.

Take for example these two example sentences, using `-` to represent an arbitrary token:

```
a b c -
a - c b
```

Assume in a prior iteration, a winning bigram was `(a, _, c)`, representing the token `a`, a gap of `1`, and then the token `c`. with a gapsize of 1. The past algorithm, run on the above corpus, would count the token `b` twice towards the same n-gram `(a, b, c)`, despite there being two distinct n-grams represented here: `(a, b, c)` and `(a, _, c, b)`.

I think the algorithm is counting on the fact that it would be very rare to encounter this sequence of lexemes in a realistic corpus, where the same word would appear within the gap **and** after the gap. I think this is more of an artifact of this specific example with an unrealistically small vocabulary.

#### References

[^1]: awahl1, MERGE. 2017. Accessed: Jul. 11, 2022. [Online]. Available: https://github.com/awahl1/MERGE

[^2]: A. Wahl and S. Th. Gries, “Multi-word Expressions: A Novel Computational Approach to Their Bottom-Up Statistical Extraction,” in Lexical Collocation Analysis, P. Cantos-Gómez and M. Almela-Sánchez, Eds. Cham: Springer International Publishing, 2018, pp. 85–109. doi: 10.1007/978-3-319-92582-0_5.

[^3]: A. Wahl, “The Distributional Learning of Multi-Word Expressions: A Computational Approach,” p. 190.

[^4]: G. Bouma, “Normalized (Pointwise) Mutual Information in Collocation Extraction,” p. 11.

[^5]: T. Dunning, “Accurate Methods for the Statistics of Surprise and Coincidence,” Computational Linguistics, vol. 19, no. 1, p. 14.
</file>

<file path="src/remerge/core.py">
import json
from dataclasses import dataclass
from enum import Enum
from functools import cached_property
from itertools import groupby
from pathlib import Path
from typing import Literal, NewType, TypeVar

from tqdm import tqdm

from ._core import Engine


class SelectionMethod(str, Enum):
    frequency = "frequency"
    log_likelihood = "log_likelihood"
    npmi = "npmi"


class TieBreaker(str, Enum):
    deterministic = "deterministic"
    legacy_first_seen = "legacy_first_seen"


class ExhaustionPolicy(str, Enum):
    stop = "stop"
    raise_ = "raise"


class NoCandidateBigramError(ValueError):
    """Raised when no candidate bigrams are available for selection."""


@dataclass(frozen=True, slots=True)
class Lexeme:
    word: tuple[str, ...]
    ix: int

    def __repr__(self) -> str:
        return f"({self.word}|{self.ix})"


LineIndex = NewType("LineIndex", int)
TokenIndex = NewType("TokenIndex", int)
Bigram = tuple[Lexeme, Lexeme]
ProgressBarOptions = Literal["all", "iterations", "none"]


@dataclass(frozen=True)
class WinnerInfo:
    bigram: Bigram
    merged_lexeme: Lexeme
    bigram_locations: list[tuple[LineIndex, TokenIndex]]

    @cached_property
    def cleaned_bigram_locations(self) -> tuple[tuple[LineIndex, TokenIndex], ...]:
        """Greedily select non-overlapping bigram starts per line."""
        clean_locations: list[tuple[LineIndex, TokenIndex]] = []
        for line, location_group in groupby(self.bigram_locations, key=lambda x: x[0]):
            exclude_tokens: set[TokenIndex] = set()
            token_ix = [i[1] for i in location_group]
            for token in token_ix:
                if token in exclude_tokens:
                    continue
                excludes = [i for i in token_ix if token <= i < token + self.n_lexemes]
                exclude_tokens.update(excludes)
                clean_locations.append((line, token))
        return tuple(clean_locations)

    def clean_bigram_locations(self) -> list[tuple[LineIndex, TokenIndex]]:
        return list(self.cleaned_bigram_locations)

    @property
    def n_lexemes(self) -> int:
        return len(self.merged_lexeme.word)

    @property
    def merge_token_count(self) -> int:
        return len(self.cleaned_bigram_locations)


StepPayload = tuple[
    float,
    list[str],
    int,
    list[str],
    int,
    list[str],
    int,
    list[tuple[int, int]],
]


EnumType = TypeVar("EnumType", bound=Enum)


def _coerce_enum(
    value: EnumType | str, enum_type: type[EnumType], argument_name: str
) -> EnumType:
    if isinstance(value, enum_type):
        return value
    try:
        return enum_type(value)
    except ValueError as exc:
        options = ", ".join(repr(option.value) for option in enum_type)
        raise ValueError(
            f"Invalid {argument_name} {value!r}. Expected one of: {options}."
        ) from exc


def _winner_from_payload(payload: StepPayload) -> tuple[WinnerInfo, float]:
    (
        selected_score,
        left_word,
        left_ix,
        right_word,
        right_ix,
        merged_word,
        merged_ix,
        bigram_locations,
    ) = payload

    winner = WinnerInfo(
        bigram=(
            Lexeme(tuple(left_word), left_ix),
            Lexeme(tuple(right_word), right_ix),
        ),
        merged_lexeme=Lexeme(tuple(merged_word), merged_ix),
        bigram_locations=[
            (LineIndex(line_ix), TokenIndex(token_ix))
            for (line_ix, token_ix) in bigram_locations
        ],
    )
    return winner, selected_score


def run(
    corpus: list[list[str]],
    iterations: int,
    *,
    method: SelectionMethod | str = SelectionMethod.log_likelihood,
    min_count: int = 0,
    output: Path | None = None,
    output_debug_each_iteration: bool = False,
    progress_bar: ProgressBarOptions = "iterations",
    tie_breaker: TieBreaker | str = TieBreaker.deterministic,
    on_exhausted: ExhaustionPolicy | str = ExhaustionPolicy.stop,
    min_score: float | None = None,
) -> list[WinnerInfo]:
    """Run the remerge algorithm."""
    method = _coerce_enum(method, SelectionMethod, "method")
    tie_breaker = _coerce_enum(tie_breaker, TieBreaker, "tie_breaker")
    on_exhausted = _coerce_enum(on_exhausted, ExhaustionPolicy, "on_exhausted")

    engine = Engine(corpus, method.value, min_count, tie_breaker.value)

    if output is not None:
        print(f"Outputting winning merged lexemes to '{output}'")

    return_progress = progress_bar in {"all", "iterations"}
    status, payloads, selected_score, corpus_length, progress_entries = engine.run(
        iterations, min_score, return_progress
    )

    if status == "no_candidate" and on_exhausted is ExhaustionPolicy.raise_:
        raise NoCandidateBigramError(
            f"No candidate bigrams available for method={method.value!r} "
            f"and min_count={min_count}."
        )

    if status == "below_min_score" and on_exhausted is ExhaustionPolicy.raise_:
        raise NoCandidateBigramError(
            f"Best candidate score ({selected_score}) is below min_score ({min_score})."
        )

    if status not in {"completed", "no_candidate", "below_min_score"}:
        raise RuntimeError(f"Unexpected engine status {status!r}.")

    winners: list[WinnerInfo] = []
    for payload in payloads:
        winner, _ = _winner_from_payload(payload)
        winners.append(winner)

    if return_progress:
        progress = tqdm(total=iterations)
        for line_hits_count, score, merged_word in progress_entries:
            pct_bgr = line_hits_count / corpus_length if corpus_length else 0.0
            progress.update(1)
            progress.set_postfix(
                {
                    "last_winner": tuple(merged_word),
                    "score": f"{score:.4g}",
                    "pct_bgr": f"{pct_bgr * 100:.1f}%",
                }
            )
        progress.close()

    if output is not None:
        if output_debug_each_iteration:
            for i in range(len(winners)):
                winner_lexemes = {
                    j: winners[j].merged_lexeme.word for j in range(i + 1)
                }
                output.write_text(json.dumps(winner_lexemes))
        else:
            winner_lexemes = {i: w.merged_lexeme.word for i, w in enumerate(winners)}
            output.write_text(json.dumps(winner_lexemes))

    return winners
</file>

</files>
